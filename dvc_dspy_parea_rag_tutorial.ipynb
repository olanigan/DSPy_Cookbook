{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olanigan/DSPy_Cookbook/blob/main/dvc_dspy_parea_rag_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApdDiy9alAzN"
      },
      "source": [
        "<center>\n",
        "    <p style=\"text-align:center\">\n",
        "        <img alt=\"parea logo\" src=\"https://media.dev.to/cdn-cgi/image/width=320,height=320,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Forganization%2Fprofile_image%2F8067%2Fc508b9f7-50ae-43b6-91fc-d8535102b518.png\" width=\"200\"/>\n",
        "        <br>\n",
        "        <a href=\"https://docs.parea.ai/\">Docs</a>\n",
        "        |\n",
        "        <a href=\"https://github.com/parea-ai/parea-sdk-py\">GitHub</a>\n",
        "        |\n",
        "        <a href=\"https://discord.gg/KbHtZqwvsQ\">Community</a>\n",
        "    </p>\n",
        "</center>\n",
        "<h1 align=\"center\">Tracing & Evaluating a DSPy Application Using Parea & DVC</h1>\n",
        "\n",
        "[DSPy](https://github.com/stanfordnlp/dspy) is a framework for automatically prompting and fine-tuning language models. It provides:\n",
        "\n",
        "- Composable and declarative APIs that allow developers to describe the architecture of their LLM application in the form of a \"module\" (inspired by PyTorch's `nn.Module`),\n",
        "- Optimizers formerly known as \"teleprompters\" that optimize a user-defined module for a particular task. The optimization could involve selecting few-shot examples, generating prompts, or fine-tuning language models.\n",
        "\n",
        "[Parea](https://www.parea.ai/) makes your DSPy applications *observable* by visualizing the underlying structure of each call to your compiled DSPy module and surfacing problematic spans of execution based on latency, token count, or other evaluation metrics. Additionally, Parea allows you to *track the performance* of your DSPy modules over time, across different architectures, optimizers, etc.\n",
        "\n",
        "[DVC's experiment tracking](https://dvc.org/doc/start/experiments/experiment-tracking) enables to associate any evaluated DSPy module with a snapshot of the workspace without polluting the git history. This enables *reproducible experiments*.\n",
        "\n",
        "In this tutorial, you will:\n",
        "- Build and optimize DSPy modules that use retrieval-augmented generation and multi-hop reasoning to answer questions over [HotPotQA](https://hotpotqa.github.io) dataset,\n",
        "- Instrument your application using [Parea AI](https://parea.ai),\n",
        "- Inspect the traces of your application to understand the inner works of a DSPy forward pass.\n",
        "- Evaluate your modules with experiments\n",
        "- Integrate with DVC to make experiments reproducible\n",
        "\n",
        "ℹ️ This notebook requires an OpenAI API key.\n",
        "\n",
        "ℹ️ This notebook requires a Parea API key, which can be created [here](https://docs.parea.ai/api-reference/authentication#parea-api-key).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTKoCQLalAzR"
      },
      "source": [
        "## 1. Install Dependencies and Import Libraries\n",
        "\n",
        "Install Parea, DSPy, DVC, and other dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFvpnKNqlAzR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7935c768-4d0c-45f4-9a97-4c462cfe6971"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting regex~=2023.10.3\n",
            "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dspy-ai\n",
            "  Downloading dspy_ai-2.4.9-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.4/220.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting parea-ai\n",
            "  Downloading parea_ai-0.2.157-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dvc\n",
            "  Downloading dvc-3.50.2-py3-none-any.whl (451 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.6/451.6 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff~=2.2.1 (from dspy-ai)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting joblib~=1.3.2 (from dspy-ai)\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai<2.0.0,>=0.28.1 (from dspy-ai)\n",
            "  Downloading openai-1.30.1-py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from dspy-ai) (2.0.3)\n",
            "Collecting ujson (from dspy-ai)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dspy-ai) (4.66.4)\n",
            "Collecting datasets<3.0.0,~=2.14.6 (from dspy-ai)\n",
            "  Downloading datasets-2.14.7-py3-none-any.whl (520 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.4/520.4 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dspy-ai) (2.31.0)\n",
            "Collecting optuna (from dspy-ai)\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic~=2.0 in /usr/local/lib/python3.10/dist-packages (from dspy-ai) (2.7.1)\n",
            "Collecting structlog (from dspy-ai)\n",
            "  Downloading structlog-24.1.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cattrs>=22.1.0 (from parea-ai)\n",
            "  Downloading cattrs-23.2.3-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting contextvars<3.0,>=2.4 (from parea-ai)\n",
            "  Downloading contextvars-2.4.tar.gz (9.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx>=0.25.0 (from parea-ai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter<2.0.0,>=1.0.0 (from parea-ai)\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Collecting levenshtein<0.26.0,>=0.25.0 (from parea-ai)\n",
            "  Downloading Levenshtein-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting poetry-plugin-dotenv<0.7.0,>=0.6.3 (from parea-ai)\n",
            "  Downloading poetry_plugin_dotenv-0.6.33-py3-none-any.whl (11 kB)\n",
            "Collecting pysbd<0.4.0,>=0.3.4 (from parea-ai)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv<2.0.0,>=1.0.1 (from parea-ai)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting pytz<2025.0,>=2024.1 (from parea-ai)\n",
            "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyupgrade<4.0.0,>=3.9.0 (from parea-ai)\n",
            "  Downloading pyupgrade-3.15.2-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken>=0.5.2 (from parea-ai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wrapt<2.0.0,>=1.16.0 (from parea-ai)\n",
            "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from dvc) (23.2.0)\n",
            "Collecting celery (from dvc)\n",
            "  Downloading celery-5.4.0-py3-none-any.whl (425 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.0/426.0 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorama>=0.3.9 (from dvc)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting configobj>=5.0.6 (from dvc)\n",
            "  Downloading configobj-5.0.8-py2.py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: distro>=1.3 in /usr/lib/python3/dist-packages (from dvc) (1.7.0)\n",
            "Collecting dpath<3,>=2.1.0 (from dvc)\n",
            "  Downloading dpath-2.1.6-py3-none-any.whl (17 kB)\n",
            "Collecting dulwich (from dvc)\n",
            "  Downloading dulwich-0.22.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (979 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m979.1/979.1 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dvc-data<3.16,>=3.15 (from dvc)\n",
            "  Downloading dvc_data-3.15.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dvc-http>=2.29.0 (from dvc)\n",
            "  Downloading dvc_http-2.32.0-py3-none-any.whl (12 kB)\n",
            "Collecting dvc-objects (from dvc)\n",
            "  Downloading dvc_objects-5.1.0-py3-none-any.whl (33 kB)\n",
            "Collecting dvc-render<2,>=1.0.1 (from dvc)\n",
            "  Downloading dvc_render-1.0.2-py3-none-any.whl (22 kB)\n",
            "Collecting dvc-studio-client<1,>=0.20 (from dvc)\n",
            "  Downloading dvc_studio_client-0.20.0-py3-none-any.whl (16 kB)\n",
            "Collecting dvc-task<1,>=0.3.0 (from dvc)\n",
            "  Downloading dvc_task-0.4.0-py3-none-any.whl (21 kB)\n",
            "Collecting flatten-dict<1,>=0.4.1 (from dvc)\n",
            "  Downloading flatten_dict-0.4.2-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting flufl.lock<8,>=5 (from dvc)\n",
            "  Downloading flufl.lock-7.1.1-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from dvc) (2023.6.0)\n",
            "Collecting funcy>=1.14 (from dvc)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Collecting grandalf<1,>=0.7 (from dvc)\n",
            "  Downloading grandalf-0.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gto<2,>=1.6.0 (from dvc)\n",
            "  Downloading gto-1.7.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.6/46.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core>=1.1 (from dvc)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting iterative-telemetry>=0.0.7 (from dvc)\n",
            "  Downloading iterative_telemetry-0.0.8-py3-none-any.whl (10 kB)\n",
            "Collecting kombu (from dvc)\n",
            "  Downloading kombu-5.3.7-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.10/dist-packages (from dvc) (3.3)\n",
            "Collecting omegaconf (from dvc)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=19 in /usr/local/lib/python3.10/dist-packages (from dvc) (24.0)\n",
            "Collecting pathspec>=0.10.3 (from dvc)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Collecting platformdirs<4,>=3.1.1 (from dvc)\n",
            "  Downloading platformdirs-3.11.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: psutil>=5.8 in /usr/local/lib/python3.10/dist-packages (from dvc) (5.9.5)\n",
            "Requirement already satisfied: pydot>=1.2.4 in /usr/local/lib/python3.10/dist-packages (from dvc) (1.4.2)\n",
            "Collecting pygtrie>=2.3.2 (from dvc)\n",
            "  Downloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: pyparsing>=2.4.7 in /usr/local/lib/python3.10/dist-packages (from dvc) (3.1.2)\n",
            "Requirement already satisfied: rich>=12 in /usr/local/lib/python3.10/dist-packages (from dvc) (13.7.1)\n",
            "Collecting ruamel.yaml>=0.17.11 (from dvc)\n",
            "  Downloading ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scmrepo<4,>=3.3.2 (from dvc)\n",
            "  Downloading scmrepo-3.3.5-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shortuuid>=0.5 (from dvc)\n",
            "  Downloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
            "Collecting shtab<2,>=1.3.4 (from dvc)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.7 in /usr/local/lib/python3.10/dist-packages (from dvc) (0.9.0)\n",
            "Collecting tomlkit>=0.11.1 (from dvc)\n",
            "  Downloading tomlkit-0.12.5-py3-none-any.whl (37 kB)\n",
            "Collecting voluptuous>=0.11.7 (from dvc)\n",
            "  Downloading voluptuous-0.14.2-py3-none-any.whl (31 kB)\n",
            "Collecting zc.lockfile>=1.2.1 (from dvc)\n",
            "  Downloading zc.lockfile-3.0.post1-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from cattrs>=22.1.0->parea-ai) (1.2.1)\n",
            "Requirement already satisfied: typing-extensions!=4.6.3,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from cattrs>=22.1.0->parea-ai) (4.11.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from configobj>=5.0.6->dvc) (1.16.0)\n",
            "Collecting immutables>=0.9 (from contextvars<3.0,>=2.4->parea-ai)\n",
            "  Downloading immutables-0.20-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (0.6)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets<3.0.0,~=2.14.6->dspy-ai)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets<3.0.0,~=2.14.6->dspy-ai)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets<3.0.0,~=2.14.6->dspy-ai)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (0.20.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,~=2.14.6->dspy-ai) (6.0.1)\n",
            "Collecting dictdiffer>=0.8.1 (from dvc-data<3.16,>=3.15->dvc)\n",
            "  Downloading dictdiffer-0.9.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting fsspec (from dvc)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.2.1 (from dvc-data<3.16,>=3.15->dvc)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sqltrie<1,>=0.11.0 (from dvc-data<3.16,>=3.15->dvc)\n",
            "  Downloading sqltrie-0.11.0-py3-none-any.whl (17 kB)\n",
            "Collecting aiohttp-retry>=2.5.0 (from dvc-http>=2.29.0->dvc)\n",
            "  Downloading aiohttp_retry-2.8.3-py3-none-any.whl (9.8 kB)\n",
            "Collecting billiard<5.0,>=4.2.0 (from celery->dvc)\n",
            "  Downloading billiard-4.2.0-py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting vine<6.0,>=5.1.0 (from celery->dvc)\n",
            "  Downloading vine-5.1.0-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: click<9.0,>=8.1.2 in /usr/local/lib/python3.10/dist-packages (from celery->dvc) (8.1.7)\n",
            "Collecting click-didyoumean>=0.3.0 (from celery->dvc)\n",
            "  Downloading click_didyoumean-0.3.1-py3-none-any.whl (3.6 kB)\n",
            "Collecting click-repl>=0.2.0 (from celery->dvc)\n",
            "  Downloading click_repl-0.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: click-plugins>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from celery->dvc) (1.1.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from celery->dvc) (2024.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from celery->dvc) (2.8.2)\n",
            "Requirement already satisfied: atpublic>=2.3 in /usr/local/lib/python3.10/dist-packages (from flufl.lock<8,>=5->dvc) (4.1.0)\n",
            "Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from gto<2,>=1.6.0->dvc) (0.9.4)\n",
            "Collecting semver>=2.13.0 (from gto<2,>=1.6.0->dvc)\n",
            "  Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from gto<2,>=1.6.0->dvc) (0.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->parea-ai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->parea-ai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.25.0->parea-ai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->parea-ai) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->parea-ai) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.25.0->parea-ai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->dvc)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from iterative-telemetry>=0.0.7->dvc) (1.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from iterative-telemetry>=0.0.7->dvc) (3.14.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter<2.0.0,>=1.0.0->parea-ai) (6.5.5)\n",
            "Collecting qtconsole (from jupyter<2.0.0,>=1.0.0->parea-ai)\n",
            "  Downloading qtconsole-5.5.2-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.4/123.4 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter<2.0.0,>=1.0.0->parea-ai) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter<2.0.0,>=1.0.0->parea-ai) (6.5.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter<2.0.0,>=1.0.0->parea-ai) (5.5.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter<2.0.0,>=1.0.0->parea-ai) (7.7.1)\n",
            "Collecting amqp<6.0.0,>=5.1.1 (from kombu->dvc)\n",
            "  Downloading amqp-5.2.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.8.0 (from levenshtein<0.26.0,>=0.25.0->parea-ai)\n",
            "  Downloading rapidfuzz-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting poetry>=1.5.1 (from poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai)\n",
            "  Downloading poetry-1.8.3-py3-none-any.whl (249 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.9/249.9 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic~=2.0->dspy-ai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic~=2.0->dspy-ai) (2.18.2)\n",
            "Collecting tokenize-rt>=5.2.0 (from pyupgrade<4.0.0,>=3.9.0->parea-ai)\n",
            "  Downloading tokenize_rt-5.2.0-py2.py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dspy-ai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dspy-ai) (2.0.7)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12->dvc) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12->dvc) (2.16.1)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.11->dvc)\n",
            "  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitpython>3 (from scmrepo<4,>=3.3.2->dvc)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygit2>=1.14.0 (from scmrepo<4,>=3.3.2->dvc)\n",
            "  Downloading pygit2-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting asyncssh<3,>=2.13.1 (from scmrepo<4,>=3.3.2->dvc)\n",
            "  Downloading asyncssh-2.14.2-py3-none-any.whl (352 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.5/352.5 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zc.lockfile>=1.2.1->dvc) (67.7.2)\n",
            "Collecting alembic>=1.5.0 (from optuna->dspy-ai)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna->dspy-ai)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna->dspy-ai) (2.0.30)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna->dspy-ai)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=39.0 in /usr/local/lib/python3.10/dist-packages (from asyncssh<3,>=2.13.1->scmrepo<4,>=3.3.2->dvc) (42.0.7)\n",
            "Requirement already satisfied: prompt-toolkit>=3.0.36 in /usr/local/lib/python3.10/dist-packages (from click-repl>=0.2.0->celery->dvc) (3.0.43)\n",
            "INFO: pip is looking at multiple versions of fsspec[http] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting fsspec[http]<=2023.10.0,>=2023.1.0 (from datasets<3.0.0,~=2.14.6->dspy-ai)\n",
            "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.4/173.4 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading fsspec-2023.9.1-py3-none-any.whl (173 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.4/173.4 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading fsspec-2023.9.0-py3-none-any.whl (173 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading fsspec-2023.4.0-py3-none-any.whl (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.0/154.0 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading fsspec-2023.3.0-py3-none-any.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.4/145.4 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of fsspec[http] to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dvc-objects (from dvc)\n",
            "  Downloading dvc_objects-5.0.0-py3-none-any.whl (33 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading dvc_objects-4.0.1-py3-none-any.whl (36 kB)\n",
            "Collecting dvc-http>=2.29.0 (from dvc)\n",
            "  Downloading dvc_http-2.31.0-py3-none-any.whl (12 kB)\n",
            "Collecting dvc-data<3.16,>=3.15 (from dvc)\n",
            "  Downloading dvc_data-3.15.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets<3.0.0,~=2.14.6 (from dspy-ai)\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dvc\n",
            "  Downloading dvc-3.50.1-py3-none-any.whl (451 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.6/451.6 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading dvc-3.50.0-py3-none-any.whl (451 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.6/451.6 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading dvc-3.49.0-py3-none-any.whl (450 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.8/450.8 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading dvc-3.48.4-py3-none-any.whl (450 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.1/450.1 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dvc-data<3.15,>=3.13 (from dvc)\n",
            "  Downloading dvc_data-3.14.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading dvc_data-3.14.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading dvc_data-3.13.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dvc\n",
            "  Downloading dvc-3.48.3-py3-none-any.whl (449 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.9/449.9 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading dvc-3.48.2-py3-none-any.whl (449 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.5/449.5 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading dvc-3.48.1-py3-none-any.whl (449 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.6/449.6 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading dvc-3.48.0-py3-none-any.whl (449 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.4/449.4 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading dvc-3.47.0-py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.9/440.9 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading dvc-3.46.0-py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading dvc-3.45.0-py3-none-any.whl (442 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.5/442.5 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading dvc-3.44.0-py3-none-any.whl (442 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.4/442.4 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dvc-data<3.12,>=3.10 (from dvc)\n",
            "  Downloading dvc_data-3.11.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading dvc_data-3.10.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dvc\n",
            "  Downloading dvc-3.43.1-py3-none-any.whl (442 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.4/442.4 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dvc-data<3.10,>=3.9 (from dvc)\n",
            "  Downloading dvc_data-3.9.0-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.3/69.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scmrepo<3,>=2.0.2 (from dvc)\n",
            "  Downloading scmrepo-2.1.1-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dvc-objects<4,>=3 (from dvc-data<3.10,>=3.9->dvc)\n",
            "  Downloading dvc_objects-3.0.6-py3-none-any.whl (36 kB)\n",
            "INFO: pip is looking at multiple versions of gto to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting gto<2,>=1.6.0 (from dvc)\n",
            "  Downloading gto-1.7.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.6/46.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gto-1.6.2-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.6/46.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,~=2.14.6->dspy-ai) (4.0.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython>3->scmrepo<4,>=3.3.2->dvc)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12->dvc) (0.1.2)\n",
            "Requirement already satisfied: build<2.0.0,>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai) (1.2.1)\n",
            "Requirement already satisfied: cachecontrol[filecache]<0.15.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai) (0.14.0)\n",
            "Collecting cleo<3.0.0,>=2.1.0 (from poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai)\n",
            "  Downloading cleo-2.1.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting crashtest<0.5.0,>=0.4.1 (from poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai)\n",
            "  Downloading crashtest-0.4.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting dulwich (from dvc-studio-client<1,>=0.20->dvc)\n",
            "  Downloading dulwich-0.21.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (514 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m514.7/514.7 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fastjsonschema<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai) (2.19.1)\n",
            "Collecting installer<0.8.0,>=0.7.0 (from poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai)\n",
            "  Downloading installer-0.7.0-py3-none-any.whl (453 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.8/453.8 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keyring<25.0.0,>=24.0.0 (from poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai)\n",
            "  Downloading keyring-24.3.1-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pexpect<5.0.0,>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai) (4.9.0)\n",
            "Collecting pkginfo<2.0,>=1.10 (from poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai)\n",
            "  Downloading pkginfo-1.10.0-py3-none-any.whl (30 kB)\n",
            "Collecting poetry-core==1.9.0 (from poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai)\n",
            "  Downloading poetry_core-1.9.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.5/309.5 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting poetry-plugin-export<2.0.0,>=1.6.0 (from poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai)\n",
            "  Downloading poetry_plugin_export-1.8.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pyproject-hooks<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai) (1.1.0)\n",
            "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shellingham<2.0,>=1.5 (from poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai) (2.0.1)\n",
            "Collecting trove-classifiers>=2022.5.19 (from poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai)\n",
            "  Downloading trove_classifiers-2024.5.17-py3-none-any.whl (13 kB)\n",
            "Collecting virtualenv<21.0.0,>=20.23.0 (from poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai)\n",
            "  Downloading virtualenv-20.26.2-py3-none-any.whl (3.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pygit2>=1.14.0->scmrepo<4,>=3.3.2->dvc) (1.16.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna->dspy-ai) (3.0.3)\n",
            "Collecting orjson (from sqltrie<1,>=0.11.0->dvc-data<3.16,>=3.15->dvc)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter<2.0.0,>=1.0.0->parea-ai) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter<2.0.0,>=1.0.0->parea-ai) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter<2.0.0,>=1.0.0->parea-ai) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter<2.0.0,>=1.0.0->parea-ai) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter<2.0.0,>=1.0.0->parea-ai) (6.3.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter<2.0.0,>=1.0.0->parea-ai) (3.6.6)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter<2.0.0,>=1.0.0->parea-ai) (3.0.10)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets<3.0.0,~=2.14.6->dspy-ai)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->parea-ai) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->parea-ai) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->parea-ai) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->parea-ai) (0.7.1)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->parea-ai) (3.1.4)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->parea-ai) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->parea-ai) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->parea-ai) (2.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->parea-ai) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->parea-ai) (0.10.0)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->parea-ai) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->parea-ai) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter<2.0.0,>=1.0.0->parea-ai) (1.3.0)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter<2.0.0,>=1.0.0->parea-ai) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter<2.0.0,>=1.0.0->parea-ai) (23.1.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter<2.0.0,>=1.0.0->parea-ai) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter<2.0.0,>=1.0.0->parea-ai) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter<2.0.0,>=1.0.0->parea-ai) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter<2.0.0,>=1.0.0->parea-ai) (0.20.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter<2.0.0,>=1.0.0->parea-ai) (1.0.0)\n",
            "Collecting qtpy>=2.4.0 (from qtconsole->jupyter<2.0.0,>=1.0.0->parea-ai)\n",
            "  Downloading QtPy-2.4.1-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: msgpack<2.0.0,>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from cachecontrol[filecache]<0.15.0,>=0.14.0->poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai) (1.0.8)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.16.0->pygit2>=1.14.0->scmrepo<4,>=3.3.2->dvc) (2.22)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython>3->scmrepo<4,>=3.3.2->dvc)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Collecting jedi>=0.16 (from ipython>=5.0.0->ipykernel->jupyter<2.0.0,>=1.0.0->parea-ai)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter<2.0.0,>=1.0.0->parea-ai) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter<2.0.0,>=1.0.0->parea-ai) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter<2.0.0,>=1.0.0->parea-ai) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter<2.0.0,>=1.0.0->parea-ai) (0.1.7)\n",
            "Collecting jaraco.classes (from keyring<25.0.0,>=24.0.0->poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai)\n",
            "  Downloading jaraco.classes-3.4.0-py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.11.4 in /usr/local/lib/python3.10/dist-packages (from keyring<25.0.0,>=24.0.0->poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai) (7.1.0)\n",
            "Requirement already satisfied: SecretStorage>=3.2 in /usr/lib/python3/dist-packages (from keyring<25.0.0,>=24.0.0->poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai) (3.3.1)\n",
            "Requirement already satisfied: jeepney>=0.4.2 in /usr/lib/python3/dist-packages (from keyring<25.0.0,>=24.0.0->poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai) (0.7.1)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter<2.0.0,>=1.0.0->parea-ai) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter<2.0.0,>=1.0.0->parea-ai) (0.2.4)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter<2.0.0,>=1.0.0->parea-ai) (4.19.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect<5.0.0,>=4.7.0->poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit>=3.0.36->click-repl>=0.2.0->celery->dvc) (0.2.13)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv<21.0.0,>=20.23.0->poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai)\n",
            "  Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter<2.0.0,>=1.0.0->parea-ai) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter<2.0.0,>=1.0.0->parea-ai) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter<2.0.0,>=1.0.0->parea-ai) (0.5.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.11.4->keyring<25.0.0,>=24.0.0->poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai) (3.18.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter<2.0.0,>=1.0.0->parea-ai) (0.8.4)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter<2.0.0,>=1.0.0->parea-ai) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter<2.0.0,>=1.0.0->parea-ai) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter<2.0.0,>=1.0.0->parea-ai) (0.18.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter<2.0.0,>=1.0.0->parea-ai) (1.8.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from jaraco.classes->keyring<25.0.0,>=24.0.0->poetry>=1.5.1->poetry-plugin-dotenv<0.7.0,>=0.6.3->parea-ai) (10.1.0)\n",
            "Building wheels for collected packages: contextvars, antlr4-python3-runtime\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-py3-none-any.whl size=7667 sha256=e37898d3eb481896fe6ccb94a6380abb683a5676ad58c001817fc9d89443e191\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/2c/c9/4b330908a23ee28818243dbd3925b0a5254f8bb9d659bf6b6a\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=7974ebb5cabbd69941285591326c9324ade52c6d1f2a527c5ba13c93719b260d\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built contextvars antlr4-python3-runtime\n",
            "Installing collected packages: trove-classifiers, pytz, pygtrie, funcy, distlib, dictdiffer, antlr4-python3-runtime, zc.lockfile, xxhash, wrapt, voluptuous, vine, ujson, tomlkit, tokenize-rt, structlog, smmap, shtab, shortuuid, shellingham, semver, ruamel.yaml.clib, regex, rapidfuzz, qtpy, python-dotenv, pysbd, poetry-core, platformdirs, pkginfo, pathspec, orjson, omegaconf, Mako, joblib, jedi, jaraco.classes, installer, immutables, h11, grandalf, flufl.lock, flatten-dict, dvc-render, dvc-objects, dulwich, dpath, diskcache, dill, crashtest, configobj, colorlog, colorama, click-didyoumean, cattrs, billiard, backoff, virtualenv, tiktoken, sqltrie, ruamel.yaml, requests-toolbelt, pyupgrade, pygit2, multiprocess, levenshtein, keyring, iterative-telemetry, hydra-core, httpcore, gitdb, dvc-studio-client, contextvars, click-repl, cleo, amqp, alembic, optuna, kombu, httpx, gitpython, dvc-data, asyncssh, aiohttp-retry, openai, dvc-http, datasets, celery, scmrepo, qtconsole, dvc-task, dspy-ai, gto, dvc, jupyter, poetry-plugin-export, poetry, poetry-plugin-dotenv, parea-ai\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2023.4\n",
            "    Uninstalling pytz-2023.4:\n",
            "      Successfully uninstalled pytz-2023.4\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.14.1\n",
            "    Uninstalling wrapt-1.14.1:\n",
            "      Successfully uninstalled wrapt-1.14.1\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2023.12.25\n",
            "    Uninstalling regex-2023.12.25:\n",
            "      Successfully uninstalled regex-2023.12.25\n",
            "  Attempting uninstall: platformdirs\n",
            "    Found existing installation: platformdirs 4.2.1\n",
            "    Uninstalling platformdirs-4.2.1:\n",
            "      Successfully uninstalled platformdirs-4.2.1\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.4.2\n",
            "    Uninstalling joblib-1.4.2:\n",
            "      Successfully uninstalled joblib-1.4.2\n",
            "  Attempting uninstall: keyring\n",
            "    Found existing installation: keyring 23.5.0\n",
            "    Uninstalling keyring-23.5.0:\n",
            "      Successfully uninstalled keyring-23.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires wrapt<1.15,>=1.11.0, but you have wrapt 1.16.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.3.5 aiohttp-retry-2.8.3 alembic-1.13.1 amqp-5.2.0 antlr4-python3-runtime-4.9.3 asyncssh-2.14.2 backoff-2.2.1 billiard-4.2.0 cattrs-23.2.3 celery-5.4.0 cleo-2.1.0 click-didyoumean-0.3.1 click-repl-0.3.0 colorama-0.4.6 colorlog-6.8.2 configobj-5.0.8 contextvars-2.4 crashtest-0.4.1 datasets-2.14.7 dictdiffer-0.9.0 dill-0.3.7 diskcache-5.6.3 distlib-0.3.8 dpath-2.1.6 dspy-ai-2.4.9 dulwich-0.21.7 dvc-3.43.1 dvc-data-3.9.0 dvc-http-2.32.0 dvc-objects-3.0.6 dvc-render-1.0.2 dvc-studio-client-0.20.0 dvc-task-0.4.0 flatten-dict-0.4.2 flufl.lock-7.1.1 funcy-2.0 gitdb-4.0.11 gitpython-3.1.43 grandalf-0.8 gto-1.6.2 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 hydra-core-1.3.2 immutables-0.20 installer-0.7.0 iterative-telemetry-0.0.8 jaraco.classes-3.4.0 jedi-0.19.1 joblib-1.3.2 jupyter-1.0.0 keyring-24.3.1 kombu-5.3.7 levenshtein-0.25.1 multiprocess-0.70.15 omegaconf-2.3.0 openai-1.30.1 optuna-3.6.1 orjson-3.10.3 parea-ai-0.2.157 pathspec-0.12.1 pkginfo-1.10.0 platformdirs-3.11.0 poetry-1.8.3 poetry-core-1.9.0 poetry-plugin-dotenv-0.6.33 poetry-plugin-export-1.8.0 pygit2-1.15.0 pygtrie-2.5.0 pysbd-0.3.4 python-dotenv-1.0.1 pytz-2024.1 pyupgrade-3.15.2 qtconsole-5.5.2 qtpy-2.4.1 rapidfuzz-3.9.1 regex-2023.10.3 requests-toolbelt-1.0.0 ruamel.yaml-0.18.6 ruamel.yaml.clib-0.2.8 scmrepo-2.1.1 semver-3.0.2 shellingham-1.5.4 shortuuid-1.0.13 shtab-1.7.1 smmap-5.0.1 sqltrie-0.11.0 structlog-24.1.0 tiktoken-0.7.0 tokenize-rt-5.2.0 tomlkit-0.12.5 trove-classifiers-2024.5.17 ujson-5.10.0 vine-5.1.0 virtualenv-20.26.2 voluptuous-0.14.2 wrapt-1.16.0 xxhash-3.4.1 zc.lockfile-3.0.post1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "contextvars",
                  "pydevd_plugins"
                ]
              },
              "id": "ef036be0bb3a4209be1728593bf5504b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install \"regex~=2023.10.3\" pygit2==1.14.1 dspy-ai parea-ai dvc  # DSPy requires an old version of regex that conflicts with the installed version on Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrtCAKsQlAzS"
      },
      "source": [
        "⚠️ DSPy conflicts with the default version of the `regex` module that comes pre-installed on Google Colab. If you are running this notebook in Google Colab, you won't need to restart the kernel after running the installation step above."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, initilize a git repository and add a commit if no git repository has been initialized. This will be necessary for the DVC integration."
      ],
      "metadata": {
        "id": "ju48BPI1xLTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git init\n",
        "!git add -A\n",
        "!git config --global user.email \"you@example.com\"\n",
        "!git config --global user.name \"Your Name\"\n",
        "!git commit -m \"Init commit\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrD9mA90x9w2",
        "outputId": "f9ef1d53-699a-4f6f-efc8-c76dc4c85f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnIWq7VQlAzS"
      },
      "source": [
        "Import libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mX_s50ChlAzS"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "from getpass import getpass\n",
        "\n",
        "import dspy\n",
        "import nest_asyncio\n",
        "import openai\n",
        "from dsp.utils import deduplicate\n",
        "from dspy import evaluate as dspy_eval\n",
        "from dspy.datasets import HotPotQA\n",
        "from dspy.teleprompt import BootstrapFewShot\n",
        "\n",
        "from parea import Parea\n",
        "from parea.helpers import TurnOffPareaLogging\n",
        "from parea.utils.trace_integrations.dspy import attach_evals_to_module, convert_dspy_examples_to_parea_dicts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P25670kglAzS"
      },
      "source": [
        "## 2. Configure Your OpenAI & Parea API Key\n",
        "\n",
        "Set your OpenAI & Parea API key if they are not already set as environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shdQmSpalAzT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6609d3bd-6076-47ee-f4fe-1fc470d8d2cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔑 Enter your Openai API key: ··········\n",
            "🔑 Enter your Parea API key: ··········\n"
          ]
        }
      ],
      "source": [
        "for api_key_name in [\"OPENAI_API_KEY\", \"PAREA_API_KEY\"]:\n",
        "    if not (api_key_value := os.getenv(api_key_name)):\n",
        "        api_key_value = getpass(f\"🔑 Enter your {api_key_name.split('_')[0].title()} API key: \")\n",
        "    if api_key_name == \"OPENAI_API_KEY\":\n",
        "        openai.api_key = api_key_value\n",
        "    os.environ[api_key_name] = api_key_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrP-kTQOlAzT"
      },
      "source": [
        "## 3. Configure LM\n",
        "\n",
        "We will use `gpt-3.5-turbo` as our LLM of choice for this tutorial. Additionally, we wil use ColBERT to retrieve Wikipedia articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0kUNUQRlAzT"
      },
      "outputs": [],
      "source": [
        "turbo = dspy.OpenAI(model=\"gpt-3.5-turbo\")\n",
        "colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
        "\n",
        "dspy.settings.configure(lm=turbo, rm=colbertv2_wiki17_abstracts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYxhydFWlAzT"
      },
      "source": [
        "## 4. Load & Index Data\n",
        "\n",
        "Next we will download the [HotPotQA](https://hotpotqa.github.io) dataset and mark the `question` field as the input field. Then, we can split the data into a training and test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.42k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "468bb1a89ca14896a50d4bc74a990885"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/9.19k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f71fe132b514982b6fea1b0d01528be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4280b6dbe3084078b75227ac3e3bab36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/566M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "859094a7774942a59eba9e243908d323"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/47.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68c929b39484438db9f4fc9d51cc8656"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/46.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41f6319702f149c9b3e30eb54df7851d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/90447 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d05c04d5b7ad454d92afc9dbd37973d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/7405 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "822e5adcdd49485cb0f5ab50729e5ab6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/7405 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "079cf47490de41a68cedc6752a8cf610"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "dataset = HotPotQA(train_seed=1, train_size=20, eval_seed=2023, dev_size=50, test_size=0)\n",
        "train_set = [x.with_inputs('question') for x in dataset.train]\n",
        "test_set = [x.with_inputs('question') for x in dataset.dev]\n",
        "\n",
        "len(train_set), len(test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458,
          "referenced_widgets": [
            "468bb1a89ca14896a50d4bc74a990885",
            "5d50758568fe404180f9bb1b3877e3f1",
            "1198437338414197a8cab445ac281c50",
            "9a491fa2eb854cb2a9c83ff095f93491",
            "2e19a1d009d04e75a3159058a94793cd",
            "df4582daebb7430c8093690207f88bb8",
            "c78c009784344fa1885f2dd75eecca59",
            "1be1d921d205432fbce760e51724de45",
            "9114b2d4fbaf43daa4ba0094763b22c4",
            "e926e79506fc43f2a44b8b44ec4dd9fd",
            "1507bc2e124440c8b22207a3f2c4fb65",
            "7f71fe132b514982b6fea1b0d01528be",
            "38bc1628934f4ce68d31b82f0a26ab32",
            "cac4ace4076d4c5c9e00ffb731adcb21",
            "8e9a314e19db40d4b0fbc68e5345985e",
            "a55b1098a87349e0837f33e93db93c42",
            "cc7618445a6f46d1b289ae0fe0aa1997",
            "cd0b61507cba4fb38a6d617e08a2b249",
            "119ed755791543fca286d5199422622e",
            "990d2dd9cc334f66995428424840257e",
            "c5af229c9469459999bfc3e42918aa2d",
            "4830bc350ebf4ab4802ef0ace24cfbdb",
            "4280b6dbe3084078b75227ac3e3bab36",
            "71280f33b707425ab21f646ab4da1e31",
            "6559bc5295734dbfbdaac53e12bd16ac",
            "522f465f49844c168772201074bd997f",
            "0756cda935cf4bb7a944da32cb714d7e",
            "6c9ee809293c4d5dab470823c08894cb",
            "9b29397047154f119717a67a6ad63ff1",
            "060d98e31b304a56b90e3cc5c757b4ed",
            "85304aa746bc4fa1a6db4ec92aade67d",
            "5c07210638a349988fa049062303ee74",
            "589466d45c1548c195f9e4fb0dd1b815",
            "859094a7774942a59eba9e243908d323",
            "46312cfc2e2146e1bde64c75cc49fb73",
            "82e337a2dc29442d8dcd01c8a7e4ce6d",
            "e8c52cbad550429bba5ce57744106451",
            "a3e7cba240fa4f9498ba765f2070e22c",
            "0b5ea677e2914d2ebf53c23b07b25ade",
            "2aa741bb85af4a009b105afce6298941",
            "9eb55cf0e2d141edbc13ab9da428f767",
            "ca6f3302b80d4f99b02d36cb78e25e33",
            "c92a0c34de344c41ac161969ee305d3c",
            "9ca3899e33a344e0a97cdd5f312d15ab",
            "68c929b39484438db9f4fc9d51cc8656",
            "44080c49d6f74a718532f894d60e37f1",
            "12ca8bf62af84e6eb10ed333bd1a7cf0",
            "96371fd1e2f44faeb1a0498bd6aa1203",
            "b88a88a46ce8468ebecfe85304386065",
            "4424dfd6dbde47d3b0812de86f572844",
            "f6912ff9996a4389923952fc66017d9e",
            "98e4008061a246959c9b8023df469b57",
            "c183c4b3464941ffb5dd09e9cc70c5a1",
            "209cbcdef7ae4e9895c03b09a1cbebd1",
            "18a1ef4780a248f9b3246ed0da753779",
            "41f6319702f149c9b3e30eb54df7851d",
            "0f1724daa0384f14b5a76b7686830316",
            "75b91ac2e42046f9a6935a52c91ff519",
            "86859c0080cc47b998c88aa733380770",
            "13dd099e80924ff2b47fb665c89969e8",
            "9c414e1321de4a0ab231df4955e66b27",
            "d9eb9fbb14bb4a24bf8f06342670fbe2",
            "09e38094499447299126a0b4fa76c595",
            "50f952c9860240d5b47cc2567cd7f7b8",
            "ec75d9edc820497097db04ad1ca1521c",
            "39c0f361a832480cb7eaa0167d4da680",
            "d05c04d5b7ad454d92afc9dbd37973d9",
            "5f47f9d97abe4da9be8186c4c2466a83",
            "ee49f77136894b42a84271a6806aab91",
            "5b39e967cbef43f99289df573bd40d28",
            "066168b3af604ec2a9e737e00b5b14f9",
            "29b5925bc04f47b8bb815fee1fcf806b",
            "9381488ddc524085a0c4e060f1c2c619",
            "289d62ae04a64afcad189ca4fc08bc08",
            "878c05819ebd464093bfe729c12d5236",
            "7f4de3ac87f040568bbadc8649e6950b",
            "b5be4b6303ef451f8260ff4afba56a20",
            "822e5adcdd49485cb0f5ab50729e5ab6",
            "6d536f7911a34106a1ab4e707fd4d587",
            "052a96e6a0274342879a650ab1970e9e",
            "0446ffbabf164fe19673ac2789764437",
            "9cad7424fd96471a93861451df98b267",
            "dc9009547c37420f8e82fd4fc1a9900f",
            "54a83b17ed32485bbfd784ddb7671693",
            "7afcac28322742cd9fc49c9d0a706ee6",
            "e1de3ce54e934624b704bf4f05048b38",
            "fe1c72dc2ca244179e3a34caacaf17bd",
            "8aa6143166444f4ab198b55135129626",
            "079cf47490de41a68cedc6752a8cf610",
            "5028d2bcd5fe47808519f76fdd873440",
            "76a43c666f214cd289f19a1b721b156d",
            "79c027727ee2439fb6710af6b06832ee",
            "cbb38b2fd9c848629f20d8bdcf156358",
            "11c36e04e63d4ea6b63402721ee4ef2e",
            "09d17ea4ec6143e4a46e10abd7121a80",
            "85e37d432cdb4a8eaae2cbb26b5b33a0",
            "2b86b299d91248789906139533337fbd",
            "71faaa24b36248998c07a1153e07d3a9",
            "5208ea6aaf66400f87b2aa027a1dda0b"
          ]
        },
        "id": "4YTTUR60ep8c",
        "outputId": "5a9cf6e6-df45-46c7-8ae5-98c605af33de"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWNZJ-DYlAzT"
      },
      "source": [
        "Each sample in our dataset has a question, and an human-annotated answer. The test set also comes with the correct Wikipedia articles to answer the question. This information will be helpful to evaluate the retrieval step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSKnwCbDlAzU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec1ca04d-0aff-4de4-f1fc-4f6562c130fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Example({'question': 'At My Window was released by which American singer-songwriter?', 'answer': 'John Townes Van Zandt'}) (input_keys={'question'}),\n",
              " Example({'question': 'Are both Cangzhou and Qionghai in the Hebei province of China?', 'answer': 'no', 'gold_titles': {'Qionghai', 'Cangzhou'}}) (input_keys={'question'}))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "train_set[0], test_set[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGCENtK8lAzU"
      },
      "source": [
        "## 5. Define A Simple RAG Module\n",
        "\n",
        "In order to define the RAG module, we need to define a signature that takes in two inputs, `context` and `question`, and outputs an `answer`. The signature provides:\n",
        "\n",
        "- A description of the sub-task the language model is supposed to solve.\n",
        "- A description of the input fields to the language model.\n",
        "- A description of the output fields the language model must produce."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oX90tzXSlAzU"
      },
      "outputs": [],
      "source": [
        "class GenerateAnswer(dspy.Signature):\n",
        "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
        "\n",
        "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
        "    question = dspy.InputField()\n",
        "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cot = dspy.ChainOfThought(GenerateAnswer)\n",
        "cot(\n",
        "    question='When was OpenAI founded?',\n",
        "    context='OpenAI is an American artificial intelligence (AI) research organization founded in December 2015'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkYHgwN6UVIP",
        "outputId": "c3627218-3a32-4ac3-83d5-ce527df03eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prediction(\n",
              "    rationale='produce the answer. We know that OpenAI is an American AI research organization.',\n",
              "    answer='December 2015'\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYJwF706lAzU"
      },
      "source": [
        "Define your module by subclassing `dspy.Module` and overriding the `forward` method. Here, we use ChromaDB to retrieve the top-k passages from the context and then use the Chain-of-Thought generate the final answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjiQA55nlAzU"
      },
      "outputs": [],
      "source": [
        "class RAG(dspy.Module):\n",
        "    def __init__(self, num_passages=3):\n",
        "        super().__init__()\n",
        "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
        "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
        "\n",
        "    def forward(self, question):\n",
        "        context = self.retrieve(question).passages\n",
        "        prediction = self.generate_answer(context=context, question=question)\n",
        "        return dspy.Prediction(context=context, answer=prediction.answer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline = RAG()\n",
        "rag_pipeline(question='When was OpenAI founded?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "793q9PqnbNBd",
        "outputId": "76b7c96e-06d8-45cf-b518-a34c4dea386f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prediction(\n",
              "    context=['Budapest Open Access Initiative | The Budapest Open Access Initiative (BOAI) is a public statement of principles relating to open access to the research literature, which was released to the public February 14, 2002. It arose from a conference convened in Budapest by the Open Society Institute on December 1–2, 2001 to promote open access – at the time also known as \"Free Online Scholarship\". This small gathering of individuals is recognised as one of the major defining events of the open access movement. On the occasion of the 10th anniversary of the initiative, it was reaffirmed in 2012 and supplemented with a set of concrete recommendations for achieving \"the new goal that within the next ten years, OA will become the default method for distributing new peer-reviewed research in every field and country.\"', 'OpenAI | OpenAI is a non-profit artificial intelligence (AI) research company that aims to promote and develop friendly AI in such a way as to benefit humanity as a whole. The organization aims to \"freely collaborate\" with other institutions and researchers by making its patents and research open to the public. The founders (notably Elon Musk and Sam Altman) are motivated in part by concerns about existential risk from artificial general intelligence.', 'OPEN (magazine) | OPEN is an Indian English-language weekly magazine. Founded by Sandipan Deb, former executive editor of \"Outlook\" and editor of \"Financial Express\", it was launched on 2 April 2009 in 12 Indian cities. The magazine is the flagship brand of Open Media Network, the media venture of RPG Group.'],\n",
              "    answer='2015'\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Evaluate the RAG Module\n",
        "\n",
        "We will use Parea to evaluate the RAG module on the test set. This consists of two parts:\n",
        "- **instrumentation**: We will trace the execution of the module components to understand how the module processes the input: done by the `trace_dspy` method.\n",
        "- **experimentation**: We will run an experiment to see the model's performance on the test set.\n",
        "\n",
        "To be able to execute experiments in a notebook, we need to enable nested asyncio loops with the help of the `nest_asyncio` module."
      ],
      "metadata": {
        "collapsed": false,
        "id": "J2LkwTi6ep8d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "p = Parea(api_key=os.getenv(\"PAREA_API_KEY\"))\n",
        "p.trace_dspy()\n",
        "\n",
        "nest_asyncio.apply()  # needed to make p.experiment work in notebooks\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # needed because of transformers"
      ],
      "metadata": {
        "id": "eZIFsWE-ep8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, we will integrate Parea with DVC's experiment tracking."
      ],
      "metadata": {
        "id": "eLJ-frLjwi92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!dvc init  # initializes DVC\n",
        "!parea dvc-init  # initializes Parea integration with DVC for experimetn tracking\n",
        "!git add .parea/metrics.json .parea/dvc.yaml && git commit -m \"Parea DVC integration\"  # files which need to be tracked in git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ipp9YPQmw0pN",
        "outputId": "dae57f01-5639-49fc-827a-69ef6c3d5ada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized DVC repository.\n",
            "\n",
            "You can now commit the changes to git.\n",
            "\n",
            "\u001b[31m+---------------------------------------------------------------------+\n",
            "\u001b[0m\u001b[31m|\u001b[0m                                                                     \u001b[31m|\u001b[0m\n",
            "\u001b[31m|\u001b[0m        DVC has enabled anonymous aggregate usage analytics.         \u001b[31m|\u001b[0m\n",
            "\u001b[31m|\u001b[0m     Read the analytics documentation (and how to opt-out) here:     \u001b[31m|\u001b[0m\n",
            "\u001b[31m|\u001b[0m             <\u001b[36mhttps://dvc.org/doc/user-guide/analytics\u001b[39m>              \u001b[31m|\u001b[0m\n",
            "\u001b[31m|\u001b[0m                                                                     \u001b[31m|\u001b[0m\n",
            "\u001b[31m+---------------------------------------------------------------------+\n",
            "\u001b[0m\n",
            "\u001b[33mWhat's next?\u001b[39m\n",
            "\u001b[33m------------\u001b[39m\n",
            "- Check out the documentation: <\u001b[36mhttps://dvc.org/doc\u001b[39m>\n",
            "- Get help and share ideas: <\u001b[36mhttps://dvc.org/chat\u001b[39m>\n",
            "- Star us on GitHub: <\u001b[36mhttps://github.com/iterative/dvc\u001b[39m>\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use two evaluation functions for our experiment:\n",
        "- `dspy.evaluate.answer_exact_match`: checks if the predicted answer is an exact match with the target answer.\n",
        "- `gold_passages_retrieved`: checks if the retrieved context matches the golden context.\n",
        "\n",
        "Note, we need to convert the list of `dspy.Example`s into a list of dictionaries and also attach the evaluation metric to the module such that we can execute the experiment with Parea. We can do the former via `convert_dspy_examples_to_parea_dicts` and the latter via `attach_evals_to_module`."
      ],
      "metadata": {
        "collapsed": false,
        "id": "etUo9qbkep8d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:04<00:00, 12.30it/s]\n",
            "0it [00:04, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment HotPotQA Run naive-rag2 stats:\n",
            "{\n",
            "  \"latency\": \"0.79\",\n",
            "  \"input_tokens\": \"0.00\",\n",
            "  \"output_tokens\": \"0.00\",\n",
            "  \"total_tokens\": \"0.00\",\n",
            "  \"cost\": \"0.00000\",\n",
            "  \"answer_exact_match\": \"0.54\",\n",
            "  \"gold_passages_retrieved\": \"0.26\"\n",
            "}\n",
            "\n",
            "\n",
            "View experiment & traces at: https://app.parea.ai/experiments/HotPotQA/07aea21a-3a8f-4d92-b795-17ad6113d0e2\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def gold_passages_retrieved(example, pred, trace=None):\n",
        "    gold_titles = set(map(dspy.evaluate.normalize_text, example['gold_titles']))\n",
        "    found_titles = set(map(dspy.evaluate.normalize_text, [c.split(' | ')[0] for c in pred.context]))\n",
        "\n",
        "    return gold_titles.issubset(found_titles)\n",
        "\n",
        "\n",
        "p.experiment(\n",
        "    \"HotPotQA\",  # name of the experiment\n",
        "    convert_dspy_examples_to_parea_dicts(test_set),  # dataset of the experiment\n",
        "    attach_evals_to_module(RAG(), [dspy_eval.answer_exact_match, gold_passages_retrieved]),  # function which should be evaluated\n",
        ").run(\n",
        "    \"naive-rag\"\n",
        ")  # name of the run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yo4R5UxNep8e",
        "outputId": "02aed452-12de-48fe-bb70-2ef45c271a4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can check that the DVC integration is working correctly"
      ],
      "metadata": {
        "id": "C-5Nri9C0lu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!dvc exp show"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6gKMH_i0Wct",
        "outputId": "f35b62a0-6041-44c4-ab0d-5aec9f2271d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
            " \u001b[1;30;107m \u001b[0m\u001b[1;30;107mExperiment              \u001b[0m\u001b[1;30;107m \u001b[0m \u001b[1;30;107m \u001b[0m\u001b[1;30;107mCreated \u001b[0m\u001b[1;30;107m \u001b[0m \u001b[1;30;107m \u001b[0m\u001b[1;30;107mlatency\u001b[0m\u001b[1;30;107m \u001b[0m \u001b[1;30;107m \u001b[0m\u001b[1;30;107minput_tokens\u001b[0m\u001b[1;30;107m \u001b[0m \u001b[1;30;107m \u001b[0m\u001b[1;30;107moutput_tokens\u001b[0m\u001b[1;30;107m \u001b[0m \u001b[1;30;107m \u001b[0m\u001b[1;30;107mtotal_tokens\u001b[0m\u001b[1;30;107m \u001b[0m \u001b[1;30;107m \u001b[0m\u001b[1;30;107m   cost\u001b[0m\u001b[1;30;107m \u001b[0m \u001b[1;30;107m \u001b[0m\u001b[1;30;107manswer_exact_match\u001b[0m\u001b[1;30;107m \u001b[0m \u001b[1;30;107m \u001b[0m\u001b[1;30;107mgold_passages_retrieved\u001b[0m\u001b[1;30;107m \u001b[0m \n",
            " ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
            " \u001b[1m \u001b[0m\u001b[1mworkspace               \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m-       \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m   0.79\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m        0.00\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m         0.00\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m        0.00\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m0.00000\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m              0.54\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m                   0.26\u001b[0m\u001b[1m \u001b[0m \n",
            " \u001b[1m \u001b[0m\u001b[1mmaster                  \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m03:38 PM\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m      -\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m           -\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m            -\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m           -\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m      -\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m                 -\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m                      -\u001b[0m\u001b[1m \u001b[0m \n",
            "  └── b4b8ceb [\u001b[1mnaive-rag2\u001b[0m]   03:42 PM      0.79           0.00            0.00           0.00   0.00000                 0.54                      0.26  \n",
            " ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that only in 37% of the cases the correct context is retrieved. Additionally, by looking at the relationship between the retrieval accuracy (`gold_passages_retrieved`) and the overall accuracy of our RAG pipeline (`answer_exact_match`), we can see our retrieval step is the bottleneck (e.g. both metrics agree in 90% of cases).\n",
        "\n",
        "![Simple RAG](https://drive.google.com/uc?id=1zZ-9b9PVfeeIX6fgSfqu_8NapIscpLsw)\n",
        "\n",
        "When inspecting a single sample, we can see that the retrieved context (middle red box) doesn't match the question (top red box) and the correct context (bottom red box) at all:\n",
        "\n",
        "![Bad Retrieval](https://drive.google.com/uc?id=1zBXRzKmTde4Qtd3cegSV1xAb9iUExDIu)"
      ],
      "metadata": {
        "collapsed": false,
        "id": "LO42AVwlep8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. We need better retrieval: Simplified Baleen\n",
        "\n",
        "One way to improve this to iteratively refine the query given already retrieved contexts before generating a final answer. This is encapsulated in standard NLP by multi-hop search systems, c.f. e.g. Baleen (Khattab et al., 2021). Let's try it out!\n",
        "\n",
        "For that we will introduce a new `Signature`: given some context and a question, generate a new query to find more relevant information."
      ],
      "metadata": {
        "collapsed": false,
        "id": "zLLm31kuep8e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class GenerateSearchQuery(dspy.Signature):\n",
        "    \"\"\"Write a simple search query that will help answer a complex question.\"\"\"\n",
        "\n",
        "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
        "    question = dspy.InputField()\n",
        "    query = dspy.OutputField()"
      ],
      "metadata": {
        "id": "PfNWwq9yep8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can define a simplified version of Baleen. Concretely, we will do in the `forward` pass:\n",
        "\n",
        "1. Loop `self.max_hops` times to fetch diverse contexts. In each iteration:\n",
        "    1. Generate a search query using Chain-of-Thought (the predictor at `self.generate_query[hop]`).\n",
        "    2. Then, retrieve the top-k passages using that query.\n",
        "    3. Finally, add the (deduplicated) passages to our accumulated context.\n",
        "2. After the loop, `self.generate_answer` generates an answer via CoT.\n",
        "3. Finally, return a prediction with the retrieved context and predicted answer.\n",
        "\n",
        "Note, we need to pull `ChromadbRM` outside of the module declaration to ensure that the module is pickleable, which is a requirement to optimize it later on."
      ],
      "metadata": {
        "collapsed": false,
        "id": "m4c8iVT2ep8e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class SimplifiedBaleen(dspy.Module):\n",
        "    def __init__(self, passages_per_hop=3, max_hops=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\n",
        "        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n",
        "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
        "        self.max_hops = max_hops\n",
        "\n",
        "    def forward(self, question):\n",
        "        context = []\n",
        "\n",
        "        for hop in range(self.max_hops):\n",
        "            query = self.generate_query[hop](context=context, question=question).query\n",
        "            passages = self.retrieve(query).passages\n",
        "            context = deduplicate(context + passages)\n",
        "\n",
        "        pred = self.generate_answer(context=context, question=question)\n",
        "        return dspy.Prediction(context=context, answer=pred.answer)"
      ],
      "metadata": {
        "id": "JDmJH8npep8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Optimizing the Baleen Model\n",
        "\n",
        "Now, we can apply the **magic** of DSPy and optimize our model on our training set. For that we need to select an optimizer and define an evaluation metric.\n",
        "\n",
        "As optimizer, we will choose the `BootstrapFewShot` optimizer which uses few-shot examples to boost the performance of the prompts. To evaluate the pipeline we will apply the following logic:\n",
        "1. check if the predicted answer is an exact match with the target answer\n",
        "2. check if the retrieved context matches the golden context\n",
        "3. check if the queries for the individual hops aren't too long\n",
        "4. check if the queries are sufficiently different from each other"
      ],
      "metadata": {
        "collapsed": false,
        "id": "VcHmJ4trep8e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▌      | 7/20 [00:35<01:06,  5.13s/it]\n"
          ]
        }
      ],
      "source": [
        "def validate_context_and_answer_and_hops(example, pred, trace=None):\n",
        "    if not dspy.evaluate.answer_exact_match(example, pred):\n",
        "        return False\n",
        "    if not dspy.evaluate.answer_passage_match(example, pred):\n",
        "        return False\n",
        "\n",
        "    hops = [example.question] + [outputs.query for *_, outputs in trace if \"query\" in outputs]\n",
        "\n",
        "    if max([len(h) for h in hops]) > 100:\n",
        "        return False\n",
        "    if any(dspy.evaluate.answer_exact_match_str(hops[idx], hops[:idx], frac=0.8) for idx in range(2, len(hops))):\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "teleprompter = BootstrapFewShot(metric=validate_context_and_answer_and_hops)\n",
        "with TurnOffPareaLogging():  # turn of logging during optimization\n",
        "    compiled_baleen = teleprompter.compile(SimplifiedBaleen(), teacher=SimplifiedBaleen(passages_per_hop=2), trainset=train_set)"
      ],
      "metadata": {
        "id": "uNc5GFnnep8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28d450b0-a315-4370-ac67-42e01b60df79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's compare the unoptimized with the optimized system to see if there are any improvements:"
      ],
      "metadata": {
        "collapsed": false,
        "id": "7uNYdqmrep8f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:26<00:00,  1.92it/s]\n",
            "0it [00:04, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment HotPotQA Run unoptimized-baleen stats:\n",
            "{\n",
            "  \"latency\": \"4.73\",\n",
            "  \"input_tokens\": \"0.00\",\n",
            "  \"output_tokens\": \"0.00\",\n",
            "  \"total_tokens\": \"0.00\",\n",
            "  \"cost\": \"0.00000\",\n",
            "  \"answer_exact_match\": \"0.56\",\n",
            "  \"gold_passages_retrieved\": \"0.40\"\n",
            "}\n",
            "\n",
            "\n",
            "View experiment & traces at: https://app.parea.ai/experiments/HotPotQA/e342a0a2-0558-4e99-bd2a-99e915bfc003\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 76%|███████▌  | 38/50 [00:18<00:03,  3.85it/s]ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158172, Requested 2787. Please try again in 359ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158172, Requested 2787. Please try again in 359ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158172, Requested 2787. Please try again in 359ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158172, Requested 2787. Please try again in 359ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 0.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158172, Requested 2787. Please try again in 359ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159423, Requested 2041. Please try again in 549ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159423, Requested 2041. Please try again in 549ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159423, Requested 2041. Please try again in 549ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159423, Requested 2041. Please try again in 549ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 0.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159423, Requested 2041. Please try again in 549ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158702, Requested 2920. Please try again in 608ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158702, Requested 2920. Please try again in 608ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158702, Requested 2920. Please try again in 608ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158702, Requested 2920. Please try again in 608ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 1.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158702, Requested 2920. Please try again in 608ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159443, Requested 2054. Please try again in 561ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159443, Requested 2054. Please try again in 561ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159443, Requested 2054. Please try again in 561ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159443, Requested 2054. Please try again in 561ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 0.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159443, Requested 2054. Please try again in 561ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159316, Requested 2290. Please try again in 602ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159316, Requested 2290. Please try again in 602ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159316, Requested 2290. Please try again in 602ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159316, Requested 2290. Please try again in 602ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 1.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159316, Requested 2290. Please try again in 602ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158951, Requested 2578. Please try again in 573ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158951, Requested 2578. Please try again in 573ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158951, Requested 2578. Please try again in 573ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158951, Requested 2578. Please try again in 573ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158951, Requested 2578. Please try again in 573ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n",
            "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n",
            "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158785, Requested 2787. Please try again in 589ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158785, Requested 2787. Please try again in 589ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158785, Requested 2787. Please try again in 589ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158785, Requested 2787. Please try again in 589ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 1.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158785, Requested 2787. Please try again in 589ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 1.2 seconds after 2 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159522, Requested 2790. Please try again in 867ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159522, Requested 2790. Please try again in 867ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159522, Requested 2790. Please try again in 867ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159522, Requested 2790. Please try again in 867ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159522, Requested 2790. Please try again in 867ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159117, Requested 2638. Please try again in 658ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159117, Requested 2638. Please try again in 658ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159117, Requested 2638. Please try again in 658ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159117, Requested 2638. Please try again in 658ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 0.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159117, Requested 2638. Please try again in 658ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158822, Requested 2920. Please try again in 653ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158822, Requested 2920. Please try again in 653ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158822, Requested 2920. Please try again in 653ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158822, Requested 2920. Please try again in 653ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 1.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158822, Requested 2920. Please try again in 653ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158764, Requested 2948. Please try again in 642ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158764, Requested 2948. Please try again in 642ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158764, Requested 2948. Please try again in 642ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158764, Requested 2948. Please try again in 642ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 0.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158764, Requested 2948. Please try again in 642ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n",
            "Backing off 1.5 seconds after 2 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n",
            "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 39/50 [00:23<00:14,  1.33s/it]ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158059, Requested 2290. Please try again in 130ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158059, Requested 2290. Please try again in 130ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158059, Requested 2290. Please try again in 130ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158059, Requested 2290. Please try again in 130ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 0.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158059, Requested 2290. Please try again in 130ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 157577, Requested 2787. Please try again in 136ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 157577, Requested 2787. Please try again in 136ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 157577, Requested 2787. Please try again in 136ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 157577, Requested 2787. Please try again in 136ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 157504, Requested 2863. Please try again in 137ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 157504, Requested 2863. Please try again in 137ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 3.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 157577, Requested 2787. Please try again in 136ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 0.7 seconds after 2 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n",
            "Backing off 3.9 seconds after 3 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 157504, Requested 2863. Please try again in 137ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 157504, Requested 2863. Please try again in 137ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 157504, Requested 2863. Please try again in 137ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159670, Requested 2790. Please try again in 922ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159670, Requested 2790. Please try again in 922ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159670, Requested 2790. Please try again in 922ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159670, Requested 2790. Please try again in 922ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159670, Requested 2790. Please try again in 922ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159589, Requested 2876. Please try again in 924ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159589, Requested 2876. Please try again in 924ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159589, Requested 2876. Please try again in 924ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159589, Requested 2876. Please try again in 924ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 1.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159589, Requested 2876. Please try again in 924ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159541, Requested 2948. Please try again in 933ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159541, Requested 2948. Please try again in 933ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159541, Requested 2948. Please try again in 933ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159541, Requested 2948. Please try again in 933ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n",
            "Backing off 0.9 seconds after 2 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n",
            "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:backoff:Backing off request(...) for 1.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159541, Requested 2948. Please try again in 933ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 1.4 seconds after 2 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 40/50 [00:25<00:13,  1.35s/it]ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158654, Requested 2863. Please try again in 568ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158654, Requested 2863. Please try again in 568ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158654, Requested 2863. Please try again in 568ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158654, Requested 2863. Please try again in 568ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158654, Requested 2863. Please try again in 568ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158708, Requested 2920. Please try again in 610ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158708, Requested 2920. Please try again in 610ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158708, Requested 2920. Please try again in 610ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158708, Requested 2920. Please try again in 610ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 0.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 158708, Requested 2920. Please try again in 610ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 0.4 seconds after 2 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n",
            "Backing off 0.5 seconds after 3 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159642, Requested 2748. Please try again in 896ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159642, Requested 2748. Please try again in 896ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159642, Requested 2748. Please try again in 896ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159642, Requested 2748. Please try again in 896ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159597, Requested 2790. Please try again in 895ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159597, Requested 2790. Please try again in 895ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159597, Requested 2790. Please try again in 895ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159597, Requested 2790. Please try again in 895ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 0.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159642, Requested 2748. Please try again in 896ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "INFO:backoff:Backing off request(...) for 1.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159597, Requested 2790. Please try again in 895ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159445, Requested 2876. Please try again in 870ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159445, Requested 2876. Please try again in 870ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159445, Requested 2876. Please try again in 870ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159445, Requested 2876. Please try again in 870ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159445, Requested 2876. Please try again in 870ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159425, Requested 2948. Please try again in 889ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159425, Requested 2948. Please try again in 889ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159425, Requested 2948. Please try again in 889ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159425, Requested 2948. Please try again in 889ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 3.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159425, Requested 2948. Please try again in 889ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n",
            "Backing off 1.5 seconds after 3 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n",
            "Backing off 0.6 seconds after 2 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n",
            "Backing off 3.6 seconds after 3 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 41/50 [00:27<00:14,  1.65s/it]ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159685, Requested 2906. Please try again in 971ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159685, Requested 2906. Please try again in 971ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159685, Requested 2906. Please try again in 971ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159685, Requested 2906. Please try again in 971ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 0.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159685, Requested 2906. Please try again in 971ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159672, Requested 2881. Please try again in 957ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159672, Requested 2881. Please try again in 957ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159620, Requested 2920. Please try again in 952ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159620, Requested 2920. Please try again in 952ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159672, Requested 2881. Please try again in 957ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159672, Requested 2881. Please try again in 957ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159620, Requested 2920. Please try again in 952ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159620, Requested 2920. Please try again in 952ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 0.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159672, Requested 2881. Please try again in 957ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159620, Requested 2920. Please try again in 952ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n",
            "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n",
            "Backing off 0.4 seconds after 4 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 42/50 [00:29<00:13,  1.74s/it]ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159386, Requested 2787. Please try again in 814ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159386, Requested 2787. Please try again in 814ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159386, Requested 2787. Please try again in 814ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159386, Requested 2787. Please try again in 814ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159377, Requested 2790. Please try again in 812ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159377, Requested 2790. Please try again in 812ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 7.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159386, Requested 2787. Please try again in 814ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159377, Requested 2790. Please try again in 812ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159377, Requested 2790. Please try again in 812ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 4.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159377, Requested 2790. Please try again in 812ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159196, Requested 2876. Please try again in 777ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159196, Requested 2876. Please try again in 777ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159196, Requested 2876. Please try again in 777ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159196, Requested 2876. Please try again in 777ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 1.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159196, Requested 2876. Please try again in 777ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 7.6 seconds after 4 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n",
            "Backing off 4.2 seconds after 4 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n",
            "Backing off 1.5 seconds after 3 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 43/50 [00:30<00:10,  1.50s/it]ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159553, Requested 2920. Please try again in 927ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159553, Requested 2920. Please try again in 927ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159528, Requested 2906. Please try again in 912ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159528, Requested 2906. Please try again in 912ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159553, Requested 2920. Please try again in 927ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159553, Requested 2920. Please try again in 927ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 12.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159553, Requested 2920. Please try again in 927ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159528, Requested 2906. Please try again in 912ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159528, Requested 2906. Please try again in 912ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 0.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159528, Requested 2906. Please try again in 912ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 12.8 seconds after 5 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n",
            "Backing off 0.2 seconds after 2 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 45/50 [00:32<00:05,  1.17s/it]ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159319, Requested 2948. Please try again in 850ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159319, Requested 2948. Please try again in 850ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:root:Error occurred in function basic_request, Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159319, Requested 2948. Please try again in 850ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_integrations/dspy.py\", line 94, in __call__\n",
            "    return trace(name=span_name)(wrapped)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 260, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parea/utils/trace_utils.py\", line 253, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 117, in basic_request\n",
            "    response = chat_request(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 263, in chat_request\n",
            "    return v1_cached_gpt3_turbo_request_v2_wrapped(**kwargs).model_dump()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/cache_utils.py\", line 16, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 256, in v1_cached_gpt3_turbo_request_v2_wrapped\n",
            "    return v1_cached_gpt3_turbo_request_v2(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 655, in __call__\n",
            "    return self._cached_call(args, kwargs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 598, in _cached_call\n",
            "    out, metadata = self.call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 856, in call\n",
            "    output = self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/dsp/modules/gpt3.py\", line 250, in v1_cached_gpt3_turbo_request_v2\n",
            "    return openai.chat.completions.create(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 590, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1240, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 921, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1005, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1020, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159319, Requested 2948. Please try again in 850ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "INFO:backoff:Backing off request(...) for 3.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-6Yw8jxokcWTXBqzY3Yv2pEuI on tokens per min (TPM): Limit 160000, Used 159319, Requested 2948. Please try again in 850ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 3.6 seconds after 4 tries calling function <function GPT3.request at 0x7f427a826b90> with kwargs {}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:44<00:00,  1.12it/s]\n",
            "0it [00:04, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment HotPotQA Run optimized-baleen stats:\n",
            "{\n",
            "  \"latency\": \"6.87\",\n",
            "  \"input_tokens\": \"0.00\",\n",
            "  \"output_tokens\": \"0.00\",\n",
            "  \"total_tokens\": \"0.00\",\n",
            "  \"cost\": \"0.00000\",\n",
            "  \"answer_exact_match\": \"0.66\",\n",
            "  \"gold_passages_retrieved\": \"0.54\"\n",
            "}\n",
            "\n",
            "\n",
            "View experiment & traces at: https://app.parea.ai/experiments/HotPotQA/fe60b715-13b3-426b-aeef-3223c4dc95de\n",
            "\n"
          ]
        }
      ],
      "source": [
        "p.experiment(\n",
        "    \"HotPotQA\",\n",
        "    convert_dspy_examples_to_parea_dicts(test_set),\n",
        "    attach_evals_to_module(SimplifiedBaleen(), [dspy_eval.answer_exact_match, gold_passages_retrieved]),\n",
        ").run(\"unoptimized-baleen\")\n",
        "\n",
        "p.experiment(\n",
        "    \"HotPotQA\", convert_dspy_examples_to_parea_dicts(test_set), attach_evals_to_module(compiled_baleen, [dspy_eval.answer_exact_match, gold_passages_retrieved])\n",
        ").run(\"optimized-baleen\")"
      ],
      "metadata": {
        "id": "AoX65fNmep8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52a0a975-ef35-4ee6-e2fa-d7e8a3250bf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When selecting both experiments in the overview, we can that our retrieval accuracy has increased from 40% to 53.3% and the overall accuracy has increased from 37% to 43%.\n",
        "\n",
        "![Experiments Comparison](https://drive.google.com/uc?id=1NI8_ELz-0Gyxw2VqQwz_HyuBOua_HVT2)"
      ],
      "metadata": {
        "collapsed": false,
        "id": "gxJgdlLcep8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can see all experiments also logged with DVC"
      ],
      "metadata": {
        "id": "JI--jGdK20hE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!dvc exp show"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5qR_QJg24MZ",
        "outputId": "91173e05-d477-4174-bd1b-68a6e3c5361e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
            " \u001b[1;30;107m \u001b[0m\u001b[1;30;107mExperiment                      \u001b[0m\u001b[1;30;107m \u001b[0m \u001b[1;30;107m \u001b[0m\u001b[1;30;107mCreated \u001b[0m\u001b[1;30;107m \u001b[0m \u001b[1;30;107m \u001b[0m\u001b[1;30;107mlatency\u001b[0m\u001b[1;30;107m \u001b[0m \u001b[1;30;107m \u001b[0m\u001b[1;30;107minput_tokens\u001b[0m\u001b[1;30;107m \u001b[0m \u001b[1;30;107m \u001b[0m\u001b[1;30;107moutput_tokens\u001b[0m\u001b[1;30;107m \u001b[0m \u001b[1;30;107m \u001b[0m\u001b[1;30;107mtotal_tokens\u001b[0m\u001b[1;30;107m \u001b[0m \u001b[1;30;107m \u001b[0m\u001b[1;30;107m   cost\u001b[0m\u001b[1;30;107m \u001b[0m \u001b[1;30;107m \u001b[0m\u001b[1;30;107manswer_exact_match\u001b[0m\u001b[1;30;107m \u001b[0m \u001b[1;30;107m \u001b[0m\u001b[1;30;107mgold_passages_retrieved\u001b[0m\u001b[1;30;107m \u001b[0m \n",
            " ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
            " \u001b[1m \u001b[0m\u001b[1mworkspace                       \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m-       \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m   6.87\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m        0.00\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m         0.00\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m        0.00\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m0.00000\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m              0.66\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m                   0.54\u001b[0m\u001b[1m \u001b[0m \n",
            " \u001b[1m \u001b[0m\u001b[1mmaster                          \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m03:38 PM\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m      -\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m           -\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m            -\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m           -\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m      -\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m                 -\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m                      -\u001b[0m\u001b[1m \u001b[0m \n",
            "  ├── 8e8ba72 [\u001b[1moptimized-baleen\u001b[0m]     03:55 PM      6.87           0.00            0.00           0.00   0.00000                 0.66                      0.54  \n",
            "  ├── a39137d [\u001b[1munoptimized-baleen\u001b[0m]   03:54 PM      4.73           0.00            0.00           0.00   0.00000                 0.56                      0.40  \n",
            "  └── b4b8ceb [\u001b[1mnaive-rag2\u001b[0m]           03:42 PM      0.79           0.00            0.00           0.00   0.00000                 0.54                      0.26  \n",
            " ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EplGnPmo25yj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "468bb1a89ca14896a50d4bc74a990885": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d50758568fe404180f9bb1b3877e3f1",
              "IPY_MODEL_1198437338414197a8cab445ac281c50",
              "IPY_MODEL_9a491fa2eb854cb2a9c83ff095f93491"
            ],
            "layout": "IPY_MODEL_2e19a1d009d04e75a3159058a94793cd"
          }
        },
        "5d50758568fe404180f9bb1b3877e3f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df4582daebb7430c8093690207f88bb8",
            "placeholder": "​",
            "style": "IPY_MODEL_c78c009784344fa1885f2dd75eecca59",
            "value": "Downloading builder script: 100%"
          }
        },
        "1198437338414197a8cab445ac281c50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1be1d921d205432fbce760e51724de45",
            "max": 6422,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9114b2d4fbaf43daa4ba0094763b22c4",
            "value": 6422
          }
        },
        "9a491fa2eb854cb2a9c83ff095f93491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e926e79506fc43f2a44b8b44ec4dd9fd",
            "placeholder": "​",
            "style": "IPY_MODEL_1507bc2e124440c8b22207a3f2c4fb65",
            "value": " 6.42k/6.42k [00:00&lt;00:00, 278kB/s]"
          }
        },
        "2e19a1d009d04e75a3159058a94793cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df4582daebb7430c8093690207f88bb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c78c009784344fa1885f2dd75eecca59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1be1d921d205432fbce760e51724de45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9114b2d4fbaf43daa4ba0094763b22c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e926e79506fc43f2a44b8b44ec4dd9fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1507bc2e124440c8b22207a3f2c4fb65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f71fe132b514982b6fea1b0d01528be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38bc1628934f4ce68d31b82f0a26ab32",
              "IPY_MODEL_cac4ace4076d4c5c9e00ffb731adcb21",
              "IPY_MODEL_8e9a314e19db40d4b0fbc68e5345985e"
            ],
            "layout": "IPY_MODEL_a55b1098a87349e0837f33e93db93c42"
          }
        },
        "38bc1628934f4ce68d31b82f0a26ab32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc7618445a6f46d1b289ae0fe0aa1997",
            "placeholder": "​",
            "style": "IPY_MODEL_cd0b61507cba4fb38a6d617e08a2b249",
            "value": "Downloading readme: 100%"
          }
        },
        "cac4ace4076d4c5c9e00ffb731adcb21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_119ed755791543fca286d5199422622e",
            "max": 9193,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_990d2dd9cc334f66995428424840257e",
            "value": 9193
          }
        },
        "8e9a314e19db40d4b0fbc68e5345985e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5af229c9469459999bfc3e42918aa2d",
            "placeholder": "​",
            "style": "IPY_MODEL_4830bc350ebf4ab4802ef0ace24cfbdb",
            "value": " 9.19k/9.19k [00:00&lt;00:00, 435kB/s]"
          }
        },
        "a55b1098a87349e0837f33e93db93c42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc7618445a6f46d1b289ae0fe0aa1997": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd0b61507cba4fb38a6d617e08a2b249": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "119ed755791543fca286d5199422622e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "990d2dd9cc334f66995428424840257e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c5af229c9469459999bfc3e42918aa2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4830bc350ebf4ab4802ef0ace24cfbdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4280b6dbe3084078b75227ac3e3bab36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_71280f33b707425ab21f646ab4da1e31",
              "IPY_MODEL_6559bc5295734dbfbdaac53e12bd16ac",
              "IPY_MODEL_522f465f49844c168772201074bd997f"
            ],
            "layout": "IPY_MODEL_0756cda935cf4bb7a944da32cb714d7e"
          }
        },
        "71280f33b707425ab21f646ab4da1e31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c9ee809293c4d5dab470823c08894cb",
            "placeholder": "​",
            "style": "IPY_MODEL_9b29397047154f119717a67a6ad63ff1",
            "value": "Downloading data files: 100%"
          }
        },
        "6559bc5295734dbfbdaac53e12bd16ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_060d98e31b304a56b90e3cc5c757b4ed",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_85304aa746bc4fa1a6db4ec92aade67d",
            "value": 3
          }
        },
        "522f465f49844c168772201074bd997f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c07210638a349988fa049062303ee74",
            "placeholder": "​",
            "style": "IPY_MODEL_589466d45c1548c195f9e4fb0dd1b815",
            "value": " 3/3 [00:19&lt;00:00,  4.94s/it]"
          }
        },
        "0756cda935cf4bb7a944da32cb714d7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c9ee809293c4d5dab470823c08894cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b29397047154f119717a67a6ad63ff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "060d98e31b304a56b90e3cc5c757b4ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85304aa746bc4fa1a6db4ec92aade67d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c07210638a349988fa049062303ee74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "589466d45c1548c195f9e4fb0dd1b815": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "859094a7774942a59eba9e243908d323": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_46312cfc2e2146e1bde64c75cc49fb73",
              "IPY_MODEL_82e337a2dc29442d8dcd01c8a7e4ce6d",
              "IPY_MODEL_e8c52cbad550429bba5ce57744106451"
            ],
            "layout": "IPY_MODEL_a3e7cba240fa4f9498ba765f2070e22c"
          }
        },
        "46312cfc2e2146e1bde64c75cc49fb73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b5ea677e2914d2ebf53c23b07b25ade",
            "placeholder": "​",
            "style": "IPY_MODEL_2aa741bb85af4a009b105afce6298941",
            "value": "Downloading data: 100%"
          }
        },
        "82e337a2dc29442d8dcd01c8a7e4ce6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9eb55cf0e2d141edbc13ab9da428f767",
            "max": 566426227,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca6f3302b80d4f99b02d36cb78e25e33",
            "value": 566426227
          }
        },
        "e8c52cbad550429bba5ce57744106451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c92a0c34de344c41ac161969ee305d3c",
            "placeholder": "​",
            "style": "IPY_MODEL_9ca3899e33a344e0a97cdd5f312d15ab",
            "value": " 566M/566M [00:15&lt;00:00, 40.5MB/s]"
          }
        },
        "a3e7cba240fa4f9498ba765f2070e22c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b5ea677e2914d2ebf53c23b07b25ade": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2aa741bb85af4a009b105afce6298941": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9eb55cf0e2d141edbc13ab9da428f767": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca6f3302b80d4f99b02d36cb78e25e33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c92a0c34de344c41ac161969ee305d3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ca3899e33a344e0a97cdd5f312d15ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68c929b39484438db9f4fc9d51cc8656": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44080c49d6f74a718532f894d60e37f1",
              "IPY_MODEL_12ca8bf62af84e6eb10ed333bd1a7cf0",
              "IPY_MODEL_96371fd1e2f44faeb1a0498bd6aa1203"
            ],
            "layout": "IPY_MODEL_b88a88a46ce8468ebecfe85304386065"
          }
        },
        "44080c49d6f74a718532f894d60e37f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4424dfd6dbde47d3b0812de86f572844",
            "placeholder": "​",
            "style": "IPY_MODEL_f6912ff9996a4389923952fc66017d9e",
            "value": "Downloading data: 100%"
          }
        },
        "12ca8bf62af84e6eb10ed333bd1a7cf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98e4008061a246959c9b8023df469b57",
            "max": 47454698,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c183c4b3464941ffb5dd09e9cc70c5a1",
            "value": 47454698
          }
        },
        "96371fd1e2f44faeb1a0498bd6aa1203": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_209cbcdef7ae4e9895c03b09a1cbebd1",
            "placeholder": "​",
            "style": "IPY_MODEL_18a1ef4780a248f9b3246ed0da753779",
            "value": " 47.5M/47.5M [00:01&lt;00:00, 41.7MB/s]"
          }
        },
        "b88a88a46ce8468ebecfe85304386065": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4424dfd6dbde47d3b0812de86f572844": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6912ff9996a4389923952fc66017d9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98e4008061a246959c9b8023df469b57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c183c4b3464941ffb5dd09e9cc70c5a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "209cbcdef7ae4e9895c03b09a1cbebd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18a1ef4780a248f9b3246ed0da753779": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41f6319702f149c9b3e30eb54df7851d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f1724daa0384f14b5a76b7686830316",
              "IPY_MODEL_75b91ac2e42046f9a6935a52c91ff519",
              "IPY_MODEL_86859c0080cc47b998c88aa733380770"
            ],
            "layout": "IPY_MODEL_13dd099e80924ff2b47fb665c89969e8"
          }
        },
        "0f1724daa0384f14b5a76b7686830316": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c414e1321de4a0ab231df4955e66b27",
            "placeholder": "​",
            "style": "IPY_MODEL_d9eb9fbb14bb4a24bf8f06342670fbe2",
            "value": "Downloading data: 100%"
          }
        },
        "75b91ac2e42046f9a6935a52c91ff519": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09e38094499447299126a0b4fa76c595",
            "max": 46213747,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_50f952c9860240d5b47cc2567cd7f7b8",
            "value": 46213747
          }
        },
        "86859c0080cc47b998c88aa733380770": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec75d9edc820497097db04ad1ca1521c",
            "placeholder": "​",
            "style": "IPY_MODEL_39c0f361a832480cb7eaa0167d4da680",
            "value": " 46.2M/46.2M [00:01&lt;00:00, 40.9MB/s]"
          }
        },
        "13dd099e80924ff2b47fb665c89969e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c414e1321de4a0ab231df4955e66b27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9eb9fbb14bb4a24bf8f06342670fbe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09e38094499447299126a0b4fa76c595": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50f952c9860240d5b47cc2567cd7f7b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec75d9edc820497097db04ad1ca1521c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39c0f361a832480cb7eaa0167d4da680": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d05c04d5b7ad454d92afc9dbd37973d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f47f9d97abe4da9be8186c4c2466a83",
              "IPY_MODEL_ee49f77136894b42a84271a6806aab91",
              "IPY_MODEL_5b39e967cbef43f99289df573bd40d28"
            ],
            "layout": "IPY_MODEL_066168b3af604ec2a9e737e00b5b14f9"
          }
        },
        "5f47f9d97abe4da9be8186c4c2466a83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29b5925bc04f47b8bb815fee1fcf806b",
            "placeholder": "​",
            "style": "IPY_MODEL_9381488ddc524085a0c4e060f1c2c619",
            "value": "Generating train split: 100%"
          }
        },
        "ee49f77136894b42a84271a6806aab91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_289d62ae04a64afcad189ca4fc08bc08",
            "max": 90447,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_878c05819ebd464093bfe729c12d5236",
            "value": 90447
          }
        },
        "5b39e967cbef43f99289df573bd40d28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f4de3ac87f040568bbadc8649e6950b",
            "placeholder": "​",
            "style": "IPY_MODEL_b5be4b6303ef451f8260ff4afba56a20",
            "value": " 90447/90447 [00:44&lt;00:00, 2817.37 examples/s]"
          }
        },
        "066168b3af604ec2a9e737e00b5b14f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29b5925bc04f47b8bb815fee1fcf806b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9381488ddc524085a0c4e060f1c2c619": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "289d62ae04a64afcad189ca4fc08bc08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "878c05819ebd464093bfe729c12d5236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f4de3ac87f040568bbadc8649e6950b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5be4b6303ef451f8260ff4afba56a20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "822e5adcdd49485cb0f5ab50729e5ab6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d536f7911a34106a1ab4e707fd4d587",
              "IPY_MODEL_052a96e6a0274342879a650ab1970e9e",
              "IPY_MODEL_0446ffbabf164fe19673ac2789764437"
            ],
            "layout": "IPY_MODEL_9cad7424fd96471a93861451df98b267"
          }
        },
        "6d536f7911a34106a1ab4e707fd4d587": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc9009547c37420f8e82fd4fc1a9900f",
            "placeholder": "​",
            "style": "IPY_MODEL_54a83b17ed32485bbfd784ddb7671693",
            "value": "Generating validation split: 100%"
          }
        },
        "052a96e6a0274342879a650ab1970e9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7afcac28322742cd9fc49c9d0a706ee6",
            "max": 7405,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1de3ce54e934624b704bf4f05048b38",
            "value": 7405
          }
        },
        "0446ffbabf164fe19673ac2789764437": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe1c72dc2ca244179e3a34caacaf17bd",
            "placeholder": "​",
            "style": "IPY_MODEL_8aa6143166444f4ab198b55135129626",
            "value": " 7405/7405 [00:02&lt;00:00, 2714.16 examples/s]"
          }
        },
        "9cad7424fd96471a93861451df98b267": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc9009547c37420f8e82fd4fc1a9900f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54a83b17ed32485bbfd784ddb7671693": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7afcac28322742cd9fc49c9d0a706ee6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1de3ce54e934624b704bf4f05048b38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe1c72dc2ca244179e3a34caacaf17bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aa6143166444f4ab198b55135129626": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "079cf47490de41a68cedc6752a8cf610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5028d2bcd5fe47808519f76fdd873440",
              "IPY_MODEL_76a43c666f214cd289f19a1b721b156d",
              "IPY_MODEL_79c027727ee2439fb6710af6b06832ee"
            ],
            "layout": "IPY_MODEL_cbb38b2fd9c848629f20d8bdcf156358"
          }
        },
        "5028d2bcd5fe47808519f76fdd873440": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11c36e04e63d4ea6b63402721ee4ef2e",
            "placeholder": "​",
            "style": "IPY_MODEL_09d17ea4ec6143e4a46e10abd7121a80",
            "value": "Generating test split: 100%"
          }
        },
        "76a43c666f214cd289f19a1b721b156d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85e37d432cdb4a8eaae2cbb26b5b33a0",
            "max": 7405,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b86b299d91248789906139533337fbd",
            "value": 7405
          }
        },
        "79c027727ee2439fb6710af6b06832ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71faaa24b36248998c07a1153e07d3a9",
            "placeholder": "​",
            "style": "IPY_MODEL_5208ea6aaf66400f87b2aa027a1dda0b",
            "value": " 7405/7405 [00:02&lt;00:00, 2901.00 examples/s]"
          }
        },
        "cbb38b2fd9c848629f20d8bdcf156358": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11c36e04e63d4ea6b63402721ee4ef2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09d17ea4ec6143e4a46e10abd7121a80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85e37d432cdb4a8eaae2cbb26b5b33a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b86b299d91248789906139533337fbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71faaa24b36248998c07a1153e07d3a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5208ea6aaf66400f87b2aa027a1dda0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
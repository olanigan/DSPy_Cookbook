{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olanigan/DSPy_Cookbook/blob/main/LLM_RL_With_Think_Tokens_DSPy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step-by-step explanation:**\n",
        "\n",
        "1.  **Setting up the tools:**\n",
        "\n",
        "    *   The code starts by importing \"libraries,\" which are collections of pre-written code that do specific tasks. It's like getting tools out of a toolbox.\n",
        "    *   It uses libraries like `dspy` (for working with language models), `torch` (for machine learning), `datasets` (for handling data), `transformers` (for advanced language models), and others for tasks like text comparison and splitting data.\n",
        "    *   It checks if the computer has a special processing unit called a GPU. GPUs are very good at the kind of math needed for machine learning, so if one is available, the code will use it to speed things up.\n",
        "\n",
        "2.  **Getting the practice questions and answers (the dataset):**\n",
        "\n",
        "    *   The code loads a dataset called \"PFAF750\". Think of this dataset as a big textbook full of questions (\"Prompts\") and their correct answers (\"Responses\").\n",
        "    *   It splits this textbook into two parts: a training set (to teach the model) and a testing set (to evaluate the model's performance later). It's like saving some questions for a final exam.\n",
        "\n",
        "3.  **Choosing a smart language model:**\n",
        "\n",
        "    *   The code selects a pre-trained language model called \"SmolLM2-360M\". This model is like a student who has already learned a lot about language from reading tons of text.\n",
        "    *   It also gets a \"tokenizer,\" which is a tool that helps the model understand words by breaking them down into smaller pieces it can process.\n",
        "\n",
        "4.  **Preparing the model:**\n",
        "\n",
        "    *   It \"wraps\" the pre-trained model with the `dspy` library, which makes it easier to use in the code's framework.\n",
        "    *   It sets up an \"optimizer,\" which is a tool that will help the model learn from its mistakes during training.\n",
        "\n",
        "5.  **Defining how to judge the model's answers:**\n",
        "\n",
        "    *   The code defines three helper functions:\n",
        "        *   `normalize_text`: This cleans up text by removing extra spaces, punctuation, and converting everything to lowercase. It's like tidying up a sentence before judging it.\n",
        "        *   `is_similar`: This checks if the model's answer is close enough to the correct answer, even if the wording is slightly different. It uses a \"fuzzy matching\" technique to compare the cleaned-up versions of the answers.\n",
        "        *   `check_format`: This checks if the model's answer follows a specific format: `<think> ... </think> <answer> ... </answer>`. It's like making sure the model shows its work in a specific way.\n",
        "        *   `combined_metric`: This combines the accuracy check and the format check into a single score. It gives the model a point for being accurate and a point for following the correct format, or takes away a point if it doesn't follow the format.\n",
        "\n",
        "6.  **Teaching the model to \"think\" (forward and backward reasoning):**\n",
        "\n",
        "    *   The code defines two \"modules\" using `dspy`:\n",
        "        *   `ForwardReasoning`: This module teaches the model to take a question, think about it, and then produce an answer. It uses the `<think> ... </think> <answer> ... </answer>` format to structure its response.\n",
        "        *   `BackwardReasoning`: This module teaches the model to take an answer, think backward, and try to reconstruct the question that led to that answer. It uses the same format as above.\n",
        "\n",
        "7.  **Fine-tuning the model:**\n",
        "\n",
        "    *   The code defines a function `fine_tune_model`. This function takes a prompt and the correct response, combines them, and feeds them to the model.\n",
        "    *   It then calculates how wrong the model's prediction was (the \"loss\") and uses the optimizer to adjust the model's internal settings so it will do better next time.\n",
        "    *   Importantly, it only focuses on the \"response\" part when calculating the loss, ignoring the \"prompt\" part.\n",
        "\n",
        "8.  **Preparing the training data:**\n",
        "\n",
        "    *   The code takes the training set of questions and answers and converts them into a format that `dspy` can understand.\n",
        "    *   It creates two sets of training data: `trainset_forward` (for forward reasoning) and `trainset_backward` (for backward reasoning).\n",
        "\n",
        "9.  **Setting up the \"teleprompter\":**\n",
        "\n",
        "    *   The code uses a `dspy` tool called `BootstrapFewShot`. This tool helps the model learn from a small number of examples. It's like giving the model a few hints before it starts practicing.\n",
        "    *   It tells the teleprompter to use the `combined_metric` to judge the model's performance.\n",
        "\n",
        "10. **Running the experiment:**\n",
        "\n",
        "    *   The code defines a function `adaptive_boundary_experiment_with_reverse`. This function runs the main training loop.\n",
        "    *   It goes through the training data multiple times (\"epochs\").\n",
        "    *   In each epoch, it does the following:\n",
        "        *   It \"compiles\" the `ForwardReasoning` and `BackwardReasoning` modules using the teleprompter. This is like giving the model a set of instructions on how to learn.\n",
        "        *   It loops through the training examples:\n",
        "            *   For each example, it asks the model to solve it using forward reasoning and then backward reasoning.\n",
        "            *   It prints the model's response and the backward-generated question.\n",
        "            *   It checks if the model's response is correct and follows the right format using the `combined_metric`.\n",
        "            *   If the model's response is incorrect or doesn't have the right format, it fine-tunes the model using the `fine_tune_model` function.\n",
        "            *   It does the same for the backward reasoning part.\n",
        "        *   It prints a message indicating that the epoch is completed.\n",
        "\n",
        "11. **Starting the training:**\n",
        "\n",
        "    *   Finally, the code calls the `adaptive_boundary_experiment_with_reverse` function to start the training process. It tells the code to run for 2 epochs.\n",
        "\n",
        "**In essence:**\n",
        "\n",
        "The code is teaching a language model to solve problems by showing it examples, judging its answers, and fine-tuning it based on its mistakes. It's also encouraging the model to \"think\" about the problem in a structured way (using the `<think> ... </think> <answer> ... </answer>` format) and to practice both forward and backward reasoning. The goal is to make the model better at solving problems and expressing its reasoning clearly."
      ],
      "metadata": {
        "id": "53IPQ3yP63KS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dspy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3oCMpZzxVGD",
        "outputId": "8b38c268-e5ec-4515-9172-e44d9668969e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dspy\n",
            "  Downloading dspy-2.5.43-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from dspy) (3.7.1)\n",
            "Collecting asyncer==0.0.8 (from dspy)\n",
            "  Downloading asyncer-0.0.8-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting backoff (from dspy)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from dspy) (5.5.1)\n",
            "Collecting datasets (from dspy)\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting diskcache (from dspy)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from dspy) (0.28.1)\n",
            "Requirement already satisfied: joblib~=1.3 in /usr/local/lib/python3.11/dist-packages (from dspy) (1.4.2)\n",
            "Collecting json-repair (from dspy)\n",
            "  Downloading json_repair-0.35.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting litellm==1.53.7 (from litellm[proxy]==1.53.7->dspy)\n",
            "  Downloading litellm-1.53.7-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting magicattr~=0.1.6 (from dspy)\n",
            "  Downloading magicattr-0.1.6-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from dspy) (1.59.9)\n",
            "Collecting optuna (from dspy)\n",
            "  Downloading optuna-4.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from dspy) (2.2.2)\n",
            "Requirement already satisfied: pydantic~=2.0 in /usr/local/lib/python3.11/dist-packages (from dspy) (2.10.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from dspy) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dspy) (2.32.3)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from dspy) (9.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from dspy) (4.67.1)\n",
            "Collecting ujson (from dspy)\n",
            "  Downloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from dspy) (3.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from dspy) (3.1.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from litellm==1.53.7->litellm[proxy]==1.53.7->dspy) (3.11.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from litellm==1.53.7->litellm[proxy]==1.53.7->dspy) (8.1.8)\n",
            "Collecting httpx (from dspy)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm==1.53.7->litellm[proxy]==1.53.7->dspy) (8.6.1)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm==1.53.7->litellm[proxy]==1.53.7->dspy) (4.23.0)\n",
            "Collecting python-dotenv>=0.2.0 (from litellm==1.53.7->litellm[proxy]==1.53.7->dspy)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting tiktoken>=0.7.0 (from litellm==1.53.7->litellm[proxy]==1.53.7->dspy)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from litellm==1.53.7->litellm[proxy]==1.53.7->dspy) (0.21.0)\n",
            "Requirement already satisfied: PyJWT<3.0.0,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm[proxy]==1.53.7->dspy) (2.10.1)\n",
            "Collecting apscheduler<4.0.0,>=3.10.4 (from litellm[proxy]==1.53.7->dspy)\n",
            "  Downloading APScheduler-3.11.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting cryptography<43.0.0,>=42.0.5 (from litellm[proxy]==1.53.7->dspy)\n",
            "  Downloading cryptography-42.0.8-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting fastapi<0.112.0,>=0.111.0 (from litellm[proxy]==1.53.7->dspy)\n",
            "  Downloading fastapi-0.111.1-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting fastapi-sso<0.11.0,>=0.10.0 (from litellm[proxy]==1.53.7->dspy)\n",
            "  Downloading fastapi_sso-0.10.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting gunicorn<23.0.0,>=22.0.0 (from litellm[proxy]==1.53.7->dspy)\n",
            "  Downloading gunicorn-22.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.7 in /usr/local/lib/python3.11/dist-packages (from litellm[proxy]==1.53.7->dspy) (3.10.15)\n",
            "Collecting pynacl<2.0.0,>=1.5.0 (from litellm[proxy]==1.53.7->dspy)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)\n",
            "Collecting python-multipart<0.0.10,>=0.0.9 (from litellm[proxy]==1.53.7->dspy)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from litellm[proxy]==1.53.7->dspy) (6.0.2)\n",
            "Collecting rq (from litellm[proxy]==1.53.7->dspy)\n",
            "  Downloading rq-2.1.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting uvicorn<0.23.0,>=0.22.0 (from litellm[proxy]==1.53.7->dspy)\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio->dspy) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->dspy) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->dspy) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->dspy) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->dspy) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->dspy) (3.0.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->dspy) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->dspy) (0.8.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai->dspy) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic~=2.0->dspy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic~=2.0->dspy) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dspy) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dspy) (2.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets->dspy) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets->dspy) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->dspy) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->dspy)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets->dspy)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->dspy)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->dspy)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets->dspy) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->dspy) (24.2)\n",
            "Collecting alembic>=1.5.0 (from optuna->dspy)\n",
            "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna->dspy)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna->dspy) (2.0.37)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->dspy) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->dspy) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->dspy) (2025.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna->dspy)\n",
            "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: tzlocal>=3.0 in /usr/local/lib/python3.11/dist-packages (from apscheduler<4.0.0,>=3.10.4->litellm[proxy]==1.53.7->dspy) (5.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<43.0.0,>=42.0.5->litellm[proxy]==1.53.7->dspy) (1.17.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.53.7->dspy)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting fastapi-cli>=0.0.2 (from fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.53.7->dspy)\n",
            "  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting email_validator>=2.0.0 (from fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.53.7->dspy)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from fastapi-sso<0.11.0,>=0.10.0->litellm[proxy]==1.53.7->dspy) (3.2.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.53.7->litellm[proxy]==1.53.7->dspy) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.53.7->litellm[proxy]==1.53.7->dspy) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.53.7->litellm[proxy]==1.53.7->dspy) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.53.7->litellm[proxy]==1.53.7->dspy) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.53.7->litellm[proxy]==1.53.7->dspy) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.53.7->litellm[proxy]==1.53.7->dspy) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.53.7->litellm[proxy]==1.53.7->dspy) (1.18.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm==1.53.7->litellm[proxy]==1.53.7->dspy) (3.21.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.53.7->litellm[proxy]==1.53.7->dspy) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.53.7->litellm[proxy]==1.53.7->dspy) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.53.7->litellm[proxy]==1.53.7->dspy) (0.22.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->dspy) (1.17.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna->dspy) (3.1.1)\n",
            "Collecting redis>=3.5 (from rq->litellm[proxy]==1.53.7->dspy)\n",
            "  Downloading redis-5.2.1-py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<43.0.0,>=42.0.5->litellm[proxy]==1.53.7->dspy) (2.22)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.53.7->dspy)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.2->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.53.7->dspy) (0.15.1)\n",
            "Collecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.2->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.53.7->dspy)\n",
            "  Downloading rich_toolkit-0.13.2-py3-none-any.whl.metadata (999 bytes)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.12.0->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.53.7->dspy)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.53.7->dspy)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.53.7->dspy)\n",
            "  Downloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.53.7->dspy) (14.2)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.2->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.53.7->dspy) (13.9.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.53.7->dspy) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.2->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.53.7->dspy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.2->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.53.7->dspy) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.2->fastapi<0.112.0,>=0.111.0->litellm[proxy]==1.53.7->dspy) (0.1.2)\n",
            "Downloading dspy-2.5.43-py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.2/345.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asyncer-0.0.8-py3-none-any.whl (9.2 kB)\n",
            "Downloading litellm-1.53.7-py3-none-any.whl (6.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading magicattr-0.1.6-py2.py3-none-any.whl (4.7 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json_repair-0.35.0-py3-none-any.whl (19 kB)\n",
            "Downloading optuna-4.2.0-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.4/383.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading APScheduler-3.11.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cryptography-42.0.8-cp39-abi3-manylinux_2_28_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.111.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi_sso-0.10.0-py3-none-any.whl (16 kB)\n",
            "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading rq-2.1.0-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\n",
            "Downloading redis-5.2.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.5/261.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich_toolkit-0.13.2-py3-none-any.whl (13 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: magicattr, xxhash, uvloop, uvicorn, ujson, redis, python-multipart, python-dotenv, Mako, json-repair, httptools, gunicorn, fsspec, dnspython, diskcache, dill, colorlog, backoff, apscheduler, watchfiles, tiktoken, starlette, rq, pynacl, multiprocess, httpx, email_validator, cryptography, asyncer, alembic, rich-toolkit, optuna, litellm, fastapi-cli, datasets, fastapi, fastapi-sso, dspy\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "  Attempting uninstall: cryptography\n",
            "    Found existing installation: cryptography 43.0.3\n",
            "    Uninstalling cryptography-43.0.3:\n",
            "      Successfully uninstalled cryptography-43.0.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.3.8 alembic-1.14.1 apscheduler-3.11.0 asyncer-0.0.8 backoff-2.2.1 colorlog-6.9.0 cryptography-42.0.8 datasets-3.2.0 dill-0.3.8 diskcache-5.6.3 dnspython-2.7.0 dspy-2.5.43 email_validator-2.2.0 fastapi-0.111.1 fastapi-cli-0.0.7 fastapi-sso-0.10.0 fsspec-2024.9.0 gunicorn-22.0.0 httptools-0.6.4 httpx-0.27.2 json-repair-0.35.0 litellm-1.53.7 magicattr-0.1.6 multiprocess-0.70.16 optuna-4.2.0 pynacl-1.5.0 python-dotenv-1.0.1 python-multipart-0.0.9 redis-5.2.1 rich-toolkit-0.13.2 rq-2.1.0 starlette-0.37.2 tiktoken-0.8.0 ujson-5.10.0 uvicorn-0.22.0 uvloop-0.21.0 watchfiles-1.0.4 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "GO0AiahKwGKm",
        "outputId": "8e5dfffb-cf0b-4b20-e6b0-d8023b5979f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/605 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "2025/01/30 15:34:09 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'question': 'Encapsulate symmetry properties through group actions.', 'answer': 'Group Actions: Identify subsets of permissible transformations operating on the collection of symbols, preserving structure and relationships among members. This includes rotational symmetries, reflections, translations, and other automorphisms.'}) (input_keys={'question'}) with <function is_similar at 0x7ddce82ca020> due to expected string or bytes-like object, got 'Example'.\n",
            "  0%|          | 1/605 [00:51<8:38:36, 51.52s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "2025/01/30 15:35:00 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'question': 'Input encoding vector: [1.82, 0.61, 1.03, 0.92]\\r\\nMetadata:\\r\\n\\r\\nFractal functions: Julia Set, Barnsley Fern, Sierpinski Gasket\\r\\nDimensions: 1.8, 2.5, 1.6\\r\\nProbability distribution: 0.4, 0.3, 0.3', 'answer': 'Output: Perform calculations similar to Example 1 but now involving more sub-vectors due to different probability distribution values.'}) (input_keys={'question'}) with <function is_similar at 0x7ddce82ca020> due to expected string or bytes-like object, got 'Example'.\n",
            "  0%|          | 2/605 [01:42<8:34:45, 51.22s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "2025/01/30 15:35:56 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'question': 'Can you create a learning path designed to teach me about website design?', 'answer': '**Learning Path: Website Design**\\n**Step 1: Learn the basics of HTML and CSS.**\\nHTML is the markup language used to create the structure of a website. CSS is the style sheet language used to control the appearance of a website. You can learn the basics of HTML and CSS by taking online courses, reading tutorials, or working through books.\\n**Step 2: Choose a website builder or content management system (CMS).**\\nA website builder or CMS is a software application that makes it easy to create and manage a website without having to know any code. There are many different website builders and CMSs available, so you should choose one that is easy to use and that meets your needs.\\n**Step 3: Design your website.**\\nOnce you have chosen a website builder or CMS, you can start designing your website. You can use a pre-made template or create your own custom design. When designing your website, keep in mind the purpose of your website and the target audience you are trying to reach.\\n**Step 4: Add content to your website.**\\nOnce you have designed your website, you can start adding content. This can include text, images, videos, and other media. When adding content to your website, make sure it is relevant to your target audience and that it is well-written and error-free.\\n**Step 5: Promote your website.**\\nOnce your website is up and running, you need to promote it so that people can find it. You can promote your website by using social media, search engine optimization (SEO), and paid advertising.\\n**Experiment:**\\nOnce you have completed your website, you can conduct an experiment to test the effectiveness of your design and content. You can track the number of visitors to your website, the amount of time they spend on your website, and the actions they take on your website.'}) (input_keys={'question'}) with <function is_similar at 0x7ddce82ca020> due to expected string or bytes-like object, got 'Example'.\n",
            "  0%|          | 3/605 [02:38<8:57:23, 53.56s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "2025/01/30 15:36:50 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'question': 'Sentence: The glimmering stars painted a tapestry of light across the midnight sky. Word to encode: glimmering Fractal functions: f_light, f_sparkle, f_beauty Dimensions: 1.6, 2.2, 1.9 Probability weights: 0.4, 0.3, 0.3', 'answer': 'Apply f_light(glimmering^(1/1.6)), f_sparkle(glimmering^(1/2.2)), and f_beauty(glimmering^(1/1.9)). Weight outputs 0.4, 0.3, and 0.3. Sum weighted outputs to get P-FAF encoding vector.'}) (input_keys={'question'}) with <function is_similar at 0x7ddce82ca020> due to expected string or bytes-like object, got 'Example'.\n",
            "  1%|          | 4/605 [03:33<8:59:05, 53.82s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "  1%|          | 4/605 [04:17<10:45:22, 64.43s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected string or bytes-like object, got 'Example'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-0bc09ea5b5b7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;31m# Call the Experiment Function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m \u001b[0madaptive_boundary_experiment_with_reverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset_backward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-0bc09ea5b5b7>\u001b[0m in \u001b[0;36madaptive_boundary_experiment_with_reverse\u001b[0;34m(trainset_forward, trainset_backward, epochs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mcompiled_forward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteleprompter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mForwardReasoning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainset_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mcompiled_backward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteleprompter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBackwardReasoning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainset_backward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dspy/teleprompt/bootstrap.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, student, teacher, trainset)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_student_and_teacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_predictor_mappings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bootstrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstudent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dspy/teleprompt/bootstrap.py\u001b[0m in \u001b[0;36m_bootstrap\u001b[0;34m(self, max_bootstraps)\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mbootstrap_attempts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bootstrap_one_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m                     \u001b[0mbootstrapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexample_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dspy/teleprompt/bootstrap.py\u001b[0m in \u001b[0;36m_bootstrap_one_example\u001b[0;34m(self, example, round_idx)\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0mcurrent_error_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_error_count\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to run or to evaluate example {example} with {self.metric} due to {e}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dspy/teleprompt/bootstrap.py\u001b[0m in \u001b[0;36m_bootstrap_one_example\u001b[0;34m(self, example, round_idx)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m                     \u001b[0mmetric_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric_threshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                         \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric_val\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric_threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-0bc09ea5b5b7>\u001b[0m in \u001b[0;36mis_similar\u001b[0;34m(response, correct_response, threshold)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Fuzzy Matching Function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mSequenceMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# DSPy Module for Forward Reasoning with 'Think' Tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-0bc09ea5b5b7>\u001b[0m in \u001b[0;36mnormalize_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Text Normalization Function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnormalize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\s+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^\\w\\s]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Fuzzy Matching Function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/re/__init__.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object, got 'Example'"
          ]
        }
      ],
      "source": [
        "import dspy\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW\n",
        "import random\n",
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"TuringsSolutions/PFAF750\")\n",
        "train_data = dataset['train']\n",
        "\n",
        "# 1. Create Train/Test Split\n",
        "# Convert the 'train' split to a list of dictionaries\n",
        "train_data_list = train_data.to_pandas().to_dict(\"records\")\n",
        "\n",
        "# Now use train_test_split\n",
        "train_data_list, test_data_list = train_test_split(train_data_list, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert back to Hugging Face Dataset objects\n",
        "train_dataset = Dataset.from_dict({\"Prompt\": [d[\"Prompt\"] for d in train_data_list], \"Response\": [d[\"Response\"] for d in train_data_list]})\n",
        "test_dataset = Dataset.from_dict({\"Prompt\": [d[\"Prompt\"] for d in test_data_list], \"Response\": [d[\"Response\"] for d in test_data_list]})\n",
        "\n",
        "# Model and Tokenizer Setup\n",
        "checkpoint = \"HuggingFaceTB/SmolLM2-360M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Handle pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token or '[PAD]'\n",
        "\n",
        "hf_model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
        "hf_model.resize_token_embeddings(len(tokenizer))\n",
        "hf_model.to(device)\n",
        "\n",
        "# Wrap with HFModel\n",
        "lm = dspy.HFModel(model=checkpoint)\n",
        "dspy.settings.configure(lm=lm, tokenizer=tokenizer)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(hf_model.parameters(), lr=5e-5)\n",
        "\n",
        "# Text Normalization Function\n",
        "def normalize_text(text):\n",
        "    return re.sub(r'\\s+', ' ', re.sub(r'[^\\w\\s]', '', text)).strip().lower()\n",
        "\n",
        "# Fuzzy Matching Function\n",
        "def is_similar(response, correct_response, threshold=0.9):\n",
        "    return SequenceMatcher(None, normalize_text(response), normalize_text(correct_response)).ratio() >= threshold\n",
        "\n",
        "# DSPy Module for Forward Reasoning with 'Think' Tokens\n",
        "class ForwardReasoning(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.generate_answer = dspy.ChainOfThought(\"question -> answer\")\n",
        "\n",
        "    def forward(self, question):\n",
        "        reasoning_process = self.generate_answer(question=question)\n",
        "        return reasoning_process.answer\n",
        "\n",
        "# DSPy Module for Backward Reasoning with 'Think' Tokens\n",
        "class BackwardReasoning(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.generate_question = dspy.ChainOfThought(\"answer -> question\")\n",
        "\n",
        "    def forward(self, answer):\n",
        "        reasoning_process = self.generate_question(answer=answer)\n",
        "        return reasoning_process.question\n",
        "\n",
        "# Fine-tuning Function with 'Think' Tokens\n",
        "def fine_tune_model(prompt, correct_response):\n",
        "    hf_model.train()\n",
        "    optimizer.zero_grad()\n",
        "    combined_text = f\"{prompt} <think>{correct_response}</think> <answer>{correct_response}</answer>\"\n",
        "    inputs = tokenizer(combined_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    prompt_length = len(tokenizer(prompt, add_special_tokens=False)[\"input_ids\"])\n",
        "    labels = input_ids.clone()\n",
        "    labels[:, :prompt_length] = -100  # Ignore prompt tokens in loss calculation\n",
        "\n",
        "    outputs = hf_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Prepare the Dataset for DSPy - Use 'Prompt' and 'Response'\n",
        "trainset_forward = []\n",
        "trainset_backward = []\n",
        "\n",
        "for example in train_data_list:\n",
        "    question = example['Prompt']  # Use 'Prompt'\n",
        "    answer = example['Response']   # Use 'Response'\n",
        "\n",
        "    # Create dspy.Example objects with input/output fields\n",
        "    trainset_forward.append(dspy.Example(question=question, answer=answer).with_inputs(\"question\"))\n",
        "    trainset_backward.append(dspy.Example(question=question, answer=answer).with_inputs(\"answer\"))\n",
        "\n",
        "# Initialize the Teleprompter (Example: BootstrapFewShot)\n",
        "from dspy.teleprompt import BootstrapFewShot\n",
        "\n",
        "# Use BootstrapFewShot for few-shot optimization\n",
        "teleprompter = BootstrapFewShot(metric=is_similar, max_bootstrapped_demos=4, max_labeled_demos=4, max_rounds=1)\n",
        "\n",
        "# Update Experiment Function to Include 'Think' Tokens\n",
        "def adaptive_boundary_experiment_with_reverse(trainset_forward, trainset_backward, epochs=1):\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        compiled_forward = teleprompter.compile(ForwardReasoning(), trainset=trainset_forward)\n",
        "        compiled_backward = teleprompter.compile(BackwardReasoning(), trainset=trainset_backward)\n",
        "\n",
        "        for example, example_backward in zip(trainset_forward, trainset_backward):\n",
        "            try:\n",
        "                prompt = example.question\n",
        "                correct_response = example.answer\n",
        "\n",
        "                forward_prediction = compiled_forward(question=prompt)\n",
        "                model_response = forward_prediction\n",
        "                print(f\"Model Response (Forward): {model_response}\")\n",
        "\n",
        "                backward_prediction = compiled_backward(answer=correct_response)\n",
        "                backward_question = backward_prediction\n",
        "                print(f\"Backward Question: {backward_question}\")\n",
        "\n",
        "                formatted_response = f\"<think>{correct_response}</think> <answer>{correct_response}</answer>\"\n",
        "\n",
        "                # Fine-tune on incorrect responses\n",
        "                if not is_similar(model_response, formatted_response):\n",
        "                    loss = fine_tune_model(prompt, correct_response)\n",
        "                    print(f\"Fine-tuned on forward reasoning. Loss: {loss:.4f}\")\n",
        "\n",
        "                formatted_question = f\"<think>{prompt}</think> <answer>{prompt}</answer>\"\n",
        "                if not is_similar(backward_question, formatted_question):\n",
        "                    loss = fine_tune_model(formatted_question, prompt)\n",
        "                    print(f\"Fine-tuned on backward reasoning. Loss: {loss:.4f}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing example: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(\"Epoch completed.\")\n",
        "\n",
        "# Call the Experiment Function\n",
        "adaptive_boundary_experiment_with_reverse(trainset_forward, trainset_backward, epochs=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dspy\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW\n",
        "import random\n",
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"TuringsSolutions/PFAF750\")\n",
        "train_data = dataset['train']\n",
        "\n",
        "# 1. Create Train/Test Split\n",
        "# Convert the 'train' split to a list of dictionaries\n",
        "train_data_list = train_data.to_pandas().to_dict(\"records\")\n",
        "\n",
        "# Now use train_test_split\n",
        "train_data_list, test_data_list = train_test_split(\n",
        "    train_data_list, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Convert back to Hugging Face Dataset objects\n",
        "train_dataset = Dataset.from_dict(\n",
        "    {\n",
        "        \"Prompt\": [d[\"Prompt\"] for d in train_data_list],\n",
        "        \"Response\": [d[\"Response\"] for d in train_data_list],\n",
        "    }\n",
        ")\n",
        "test_dataset = Dataset.from_dict(\n",
        "    {\n",
        "        \"Prompt\": [d[\"Prompt\"] for d in test_data_list],\n",
        "        \"Response\": [d[\"Response\"] for d in test_data_list],\n",
        "    }\n",
        ")\n",
        "\n",
        "# Model and Tokenizer Setup\n",
        "checkpoint = \"HuggingFaceTB/SmolLM2-360M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Handle pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token or \"[PAD]\"\n",
        "\n",
        "hf_model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
        "hf_model.resize_token_embeddings(len(tokenizer))\n",
        "hf_model.to(device)\n",
        "\n",
        "# Wrap with HFModel\n",
        "lm = dspy.HFModel(model=checkpoint)\n",
        "dspy.settings.configure(lm=lm, tokenizer=tokenizer)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(hf_model.parameters(), lr=5e-5)\n",
        "\n",
        "# Text Normalization Function\n",
        "def normalize_text(text):\n",
        "    return re.sub(r\"\\s+\", \" \", re.sub(r\"[^\\w\\s]\", \"\", text)).strip().lower()\n",
        "\n",
        "# Fuzzy Matching Function\n",
        "def is_similar(response, correct_response, threshold=0.9):\n",
        "    return (\n",
        "        SequenceMatcher(None, normalize_text(response), normalize_text(correct_response)).ratio()\n",
        "        >= threshold\n",
        "    )\n",
        "\n",
        "# Format Checking Function\n",
        "def check_format(response):\n",
        "    \"\"\"Checks if the response string has the correct format.\"\"\"\n",
        "    return (\n",
        "        response.startswith(\"<think>\")\n",
        "        and \"</think> <answer>\" in response\n",
        "        and response.endswith(\"</answer>\")\n",
        "    )\n",
        "\n",
        "# Combined Metric Function\n",
        "def combined_metric(example, pred, trace=None):\n",
        "    \"\"\"\n",
        "    Calculates a combined score based on accuracy and format.\n",
        "\n",
        "    Args:\n",
        "        example: The dspy.Example object.\n",
        "        pred: The model's response dictionary.\n",
        "        trace: An optional argument for tracing information. Not used in this function.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing:\n",
        "        - accuracy_score: The accuracy score based on is_similar (0.0 or 1.0).\n",
        "        - format_score: The format score (1.0 for correct format, -1.0 for incorrect).\n",
        "        - combined_score: The combined score (accuracy_score + format_score).\n",
        "    \"\"\"\n",
        "\n",
        "    pred = pred[\"response\"]\n",
        "\n",
        "    accuracy_score = is_similar(pred, example.answer)\n",
        "    format_score = 1.0 if check_format(pred) else -1.0\n",
        "    combined_score = accuracy_score + format_score\n",
        "    return {\n",
        "        \"accuracy_score\": accuracy_score,\n",
        "        \"format_score\": format_score,\n",
        "        \"combined_score\": combined_score,\n",
        "    }\n",
        "\n",
        "# DSPy Module for Forward Reasoning with 'Think' Tokens\n",
        "class ForwardReasoning(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.generate_answer = dspy.ChainOfThought(\"question -> answer\")\n",
        "\n",
        "    def forward(self, question):\n",
        "        reasoning_process = self.generate_answer(question=question)\n",
        "        formatted_response = f\"<think>{reasoning_process.answer}</think> <answer>{reasoning_process.answer}</answer>\"\n",
        "        return {\"response\": reasoning_process.answer, \"formatted_response\": formatted_response}\n",
        "\n",
        "# DSPy Module for Backward Reasoning with 'Think' Tokens\n",
        "class BackwardReasoning(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.generate_question = dspy.ChainOfThought(\"answer -> question\")\n",
        "\n",
        "    def forward(self, answer):\n",
        "        reasoning_process = self.generate_question(answer=answer)\n",
        "        formatted_question = f\"<think>{reasoning_process.question}</think> <answer>{reasoning_process.question}</answer>\"\n",
        "        return {\"response\": reasoning_process.question, \"formatted_response\": formatted_question}\n",
        "\n",
        "# Fine-tuning Function with 'Think' Tokens\n",
        "def fine_tune_model(prompt, correct_response):\n",
        "    hf_model.train()\n",
        "    optimizer.zero_grad()\n",
        "    combined_text = f\"{prompt} {correct_response}\"\n",
        "    inputs = tokenizer(combined_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    prompt_length = len(tokenizer(prompt, add_special_tokens=False)[\"input_ids\"])\n",
        "    labels = input_ids.clone()\n",
        "    labels[:, :prompt_length] = -100  # Ignore prompt tokens in loss calculation\n",
        "\n",
        "    outputs = hf_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Prepare the Dataset for DSPy - Use 'Prompt' and 'Response'\n",
        "trainset_forward = []\n",
        "trainset_backward = []\n",
        "\n",
        "for example in train_data_list:\n",
        "    question = example[\"Prompt\"]  # Use 'Prompt'\n",
        "    answer = example[\"Response\"]  # Use 'Response'\n",
        "\n",
        "    # Create dspy.Example objects with input/output fields\n",
        "    trainset_forward.append(\n",
        "        dspy.Example(question=question, answer=answer).with_inputs(\"question\")\n",
        "    )\n",
        "    trainset_backward.append(\n",
        "        dspy.Example(question=question, answer=answer).with_inputs(\"answer\")\n",
        "    )\n",
        "\n",
        "# Initialize the Teleprompter (Example: BootstrapFewShot)\n",
        "from dspy.teleprompt import BootstrapFewShot\n",
        "\n",
        "# Use BootstrapFewShot for few-shot optimization\n",
        "teleprompter = BootstrapFewShot(\n",
        "    metric=combined_metric, max_bootstrapped_demos=4, max_labeled_demos=4, max_rounds=1\n",
        ")\n",
        "\n",
        "# Update Experiment Function to Include 'Think' Tokens\n",
        "def adaptive_boundary_experiment_with_reverse(trainset_forward, trainset_backward, epochs=1):\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        compiled_forward = teleprompter.compile(ForwardReasoning(), trainset=trainset_forward)\n",
        "        compiled_backward = teleprompter.compile(BackwardReasoning(), trainset=trainset_backward)\n",
        "\n",
        "        for example, example_backward in zip(trainset_forward, trainset_backward):\n",
        "            try:\n",
        "                prompt = example.question\n",
        "                correct_response = example.answer\n",
        "\n",
        "                forward_prediction = compiled_forward(question=prompt)\n",
        "                model_response = forward_prediction[\"response\"]\n",
        "                formatted_response = forward_prediction[\"formatted_response\"]\n",
        "                print(f\"Model Response (Forward): {model_response}\")\n",
        "\n",
        "                backward_prediction = compiled_backward(answer=correct_response)\n",
        "                backward_question = backward_prediction[\"response\"]\n",
        "                backward_formatted_question = backward_prediction[\"formatted_response\"]\n",
        "                print(f\"Backward Question: {backward_question}\")\n",
        "\n",
        "                # Fine-tune on incorrect responses or incorrect format\n",
        "                metrics = combined_metric(example, forward_prediction)\n",
        "                if metrics[\"accuracy_score\"] < 1.0 or metrics[\"format_score\"] < 0:\n",
        "                    loss = fine_tune_model(prompt, formatted_response)\n",
        "                    print(f\"Fine-tuned on forward reasoning. Loss: {loss:.4f}, Accuracy Score: {metrics['accuracy_score']}, Format Score: {metrics['format_score']}\")\n",
        "\n",
        "                backward_metrics = combined_metric(example_backward, backward_prediction)\n",
        "                if backward_metrics[\"accuracy_score\"] < 1.0 or backward_metrics[\"format_score\"] < 0:\n",
        "                    loss = fine_tune_model(correct_response, backward_formatted_question)\n",
        "                    print(f\"Fine-tuned on backward reasoning. Loss: {loss:.4f}, Accuracy Score: {backward_metrics['accuracy_score']}, Format Score: {backward_metrics['format_score']}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing example: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(\"Epoch completed.\")\n",
        "\n",
        "# Call the Experiment Function\n",
        "adaptive_boundary_experiment_with_reverse(trainset_forward, trainset_backward, epochs=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsW5KEo93dpV",
        "outputId": "489ae4a0-b9e3-4a75-fde5-60473f9f8788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/605 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "  0%|          | 1/605 [00:51<8:43:27, 52.00s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "  0%|          | 2/605 [01:43<8:37:23, 51.48s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "  0%|          | 3/605 [02:32<8:25:40, 50.40s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "  1%|          | 4/605 [03:24<8:31:47, 51.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 4 full traces after 4 examples for up to 1 rounds, amounting to 4 attempts.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/605 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "  0%|          | 1/605 [00:49<8:23:00, 49.97s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "  0%|          | 2/605 [01:36<8:03:43, 48.13s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "  0%|          | 3/605 [02:38<9:04:05, 54.23s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "  1%|          | 4/605 [03:28<8:42:36, 52.17s/it]\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 4 full traces after 4 examples for up to 1 rounds, amounting to 4 attempts.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Response (Forward): ${answer}\n",
            "\n",
            "---\n",
            "\n",
            "Question: Encapsulate symmetry properties through group actions.\n",
            "Reasoning: Let's think step by step in order to Given the fields `question`, produce the fields `answer`. --- Follow the following format. Question: ${question} Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
            "Answer: ${answer} --- Question: Imagine raising the word to a non-integer power before applying these branches. This allows us to capture nuances, like a gentle breeze for \"delicate\" or a roaring storm for \"powerful.\" Answer: (Model understands the role of dimensions in capturing different levels of meaning.) --- Question: Prove ∃x(¬P(x)) from ∀x(P(x) ⇒ Q(x)) and ∃x(¬Q(x)) using natural deduction. Answer: (Model constructs a formal proof using inference rules and assumptions.) --- Question: What role does the P-FAF function play in addressing cold start issues in recommendation systems? Answer: \"Cold start issues arise in recommendation systems when insufficient user history impedes accurate personalization. Integrating the P-FAF function can alleviate these difficulties by providing a flexible framework for injecting auxiliary information into otherwise sparse interaction matrices. For instance, side information pertaining to item categories, user demographics, or external knowledge graphs can be incorporated via tailored fractal functions. Consequently, the P-FAF function serves two primary purposes in tackling cold start problems: Augmenting scarce historical records with supplementary data sources Encoding heterogeneous information types consistently, regardless of varying availability or density levels By doing so, the P-FAF function contributes to building comprehensive user profiles and item representations, bolstering recommendations even during early stages of engagement.\" --- Question: Word: penitent Functions: f_regret, f_atonement Dimensions: 1.7, 1.9 Distribution: 0.6, 0.4 Answer: Apply f_regret and f_atonement to powered inputs penitent^(1/1.7) & penitent^(1/1.9). Weight outputs 0.6 and 0.4. Sum to encode. --- Question: Encapsulate symmetry properties through group actions. Reasoning: Let's think step by step in order to understand symmetry properties. --- Question: What is the difference between a group and a group action? Answer: A group is a set of elements together with a binary operation satisfying certain axioms. A group action is a way of associating a group with a set, allowing us to study both simultaneously. --- Question: What is the difference between a group and a group action? Answer: A group is a set of elements together with a binary operation satisfying certain axioms. A group action is a way of associating a group with a set, allowing us to study both simultaneously. --- Question: What is the difference between a group and a group action? Answer: A group is a\n",
            "\n",
            "---\n",
            "\n",
            "Question: Input encoding vector: [1.82, 0.61, 1.03, 0.92] Metadata: Fractal functions: Julia Set, Barnsley Fern, Sierpinski Gasket Dimensions: 1.8, 2.5, 1.6 Probability distribution: 0.4, 0.3, 0.3\n",
            "Reasoning: Let's think step by step in order to Given the fields `question`, produce the fields `answer`. --- Follow the following format. Question: ${question} Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
            "Answer: ${answer} --- Question: Imagine raising the word to a non-integer power before applying these branches. This allows us to capture nuances, like a gentle breeze for \"delicate\" or a roaring storm for \"powerful.\" Answer: (Model understands the role of dimensions in capturing different levels of meaning.) --- Question: Prove ∃x(¬P(x)) from ∀x(P(x) ⇒ Q(x)) and ∃x(¬Q(x)) using natural deduction. Answer: (Model constructs a formal proof using inference rules and assumptions.) --- Question: What role does the P-FAF function play in addressing cold start issues in recommendation systems? Answer: \"Cold start issues arise in recommendation systems when insufficient user history impedes accurate personalization. Integrating the P-FAF function can alleviate these difficulties by providing a flexible framework for injecting auxiliary information into otherwise sparse interaction matrices. For instance, side information pertaining to item categories, user demographics, or external knowledge graphs can be incorporated via tailored fractal functions. Consequently, the P-FAF function serves two primary purposes in tackling cold start problems: Augmenting scarce historical records with supplementary data sources Encoding heterogeneous information types consistently, regardless of varying availability or density levels By doing so, the P-FAF function contributes to building comprehensive user profiles and item representations, bolstering recommendations even during early stages of engagement.\" --- Question: Word: penitent Functions: f_regret, f_atonement Dimensions: 1.7, 1.9 Distribution: 0.6, 0.4 Answer: Apply f_regret and f_atonement to powered inputs penitent^(1/1.7) & penitent^(1/1.9). Weight outputs 0.6 and 0.4. Sum to encode. --- Question: Input encoding vector: [1.82, 0.61, 1.03, 0.92] Metadata: Fractal functions: Julia Set, Barnsley Fern, Sierpinski Gasket Dimensions: 1.8, 2.5, 1.6 Probability distribution: 0.4, 0.3, 0.3 Reasoning: Let's think step by step in order to encode the input vector. --- Question: What is the role of the fractal functions in the above example? Answer: \"The fractal functions provide a means of encoding complex patterns and structures within the input vector. Specifically, they allow us to represent the underlying structure of the input vector as a fractal pattern, thereby facilitating the extraction of latent features and relationships between variables. Moreover, the fractal functions enable us to capture subtle variations in the input vector, such as those arising from the presence of noise or outliers. By doing so, the fractal functions contribute to enhancing the interpretability and interpretability of the input vector, ultimately leading to improved model performance.\" --- Question: What is the role of\n",
            "\n",
            "---\n",
            "\n",
            "Question: Can you create a learning path designed to teach me about website design?\n",
            "Reasoning: Let's think step by step in order to Given the fields `question`, produce the fields `answer`. --- Follow the following format. Question: ${question} Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
            "Answer: ${answer} --- Question: Imagine raising the word to a non-integer power before applying these branches. This allows us to capture nuances, like a gentle breeze for \"delicate\" or a roaring storm for \"powerful.\" Answer: (Model understands the role of dimensions in capturing different levels of meaning.) --- Question: Prove ∃x(¬P(x)) from ∀x(P(x) ⇒ Q(x)) and ∃x(¬Q(x)) using natural deduction. Answer: (Model constructs a formal proof using inference rules and assumptions.) --- Question: What role does the P-FAF function play in addressing cold start issues in recommendation systems? Answer: \"Cold start issues arise in recommendation systems when insufficient user history impedes accurate personalization. Integrating the P-FAF function can alleviate these difficulties by providing a flexible framework for injecting auxiliary information into otherwise sparse interaction matrices. For instance, side information pertaining to item categories, user demographics, or external knowledge graphs can be incorporated via tailored fractal functions. Consequently, the P-FAF function serves two primary purposes in tackling cold start problems: Augmenting scarce historical records with supplementary data sources Encoding heterogeneous information types consistently, regardless of varying availability or density levels By doing so, the P-FAF function contributes to building comprehensive user profiles and item representations, bolstering recommendations even during early stages of engagement.\" --- Question: Word: penitent Functions: f_regret, f_atonement Dimensions: 1.7, 1.9 Distribution: 0.6, 0.4 Answer: Apply f_regret and f_atonement to powered inputs penitent^(1/1.7) & penitent^(1/1.9). Weight outputs 0.6 and 0.4. Sum to encode. --- Question: Can you create a learning path designed to teach me about website design? Reasoning: Let's think step by step in order to learn about website design. --- Question: What is the difference between a \"real\" and a \"fake\" person? Answer: \"A real person is someone who exists in the world, whereas a fake person is a construct created solely for entertainment purposes. Both types of persons possess distinct characteristics, behaviors, and identities. However, the distinction between them lies in their existence versus their creation. A real person is a genuine individual, while a fake person is a fabricated entity designed to elicit specific responses from audiences.\" --- Question: What is the difference between a \"real\" and a \"fake\" person? Answer: \"A real person is someone who exists in the world, whereas a fake person is\n",
            "\n",
            "---\n",
            "\n",
            "Question: Sentence: The glimmering stars painted a tapestry of light across the midnight sky. Word to encode: glimmering Fractal functions: f_light, f_sparkle, f_beauty Dimensions: 1.6, 2.2, 1.9 Probability weights: 0.4, 0.3, 0.3\n",
            "Reasoning: Let's think step by step in order to Given the fields `question`, produce the fields `answer`. --- Follow the following format. Question: ${question} Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
            "Answer: ${answer} --- Question: Imagine raising the word to a non-integer power before applying these branches. This allows us to capture nuances, like a gentle breeze for \"delicate\" or a roaring storm for \"powerful.\" Answer: (Model understands the role of dimensions in capturing different levels of meaning.) --- Question: Prove ∃x(¬P(x)) from ∀x(P(x) ⇒ Q(x)) and ∃x(¬Q(x)) using natural deduction. Answer: (Model constructs a formal proof using inference rules and assumptions.) --- Question: What role does the P-FAF function play in addressing cold start issues in recommendation systems? Answer: \"Cold start issues arise in recommendation systems when insufficient user history impedes accurate personalization. Integrating the P-FAF function can alleviate these difficulties by providing a flexible framework for injecting auxiliary information into otherwise sparse interaction matrices. For instance, side information pertaining to item categories, user demographics, or external knowledge graphs can be incorporated via tailored fractal functions. Consequently, the P-FAF function serves two primary purposes in tackling cold start problems: Augmenting scarce historical records with supplementary data sources Encoding heterogeneous information types consistently, regardless of varying availability or density levels By doing so, the P-FAF function contributes to building comprehensive user profiles and item representations, bolstering recommendations even during early stages of engagement.\" --- Question: Word: penitent Functions: f_regret, f_atonement Dimensions: 1.7, 1.9 Distribution: 0.6, 0.4 Answer: Apply f_regret and f_atonement to powered inputs penitent^(1/1.7) & penitent^(1/1.9). Weight outputs 0.6 and 0.4. Sum to encode. --- Question: Sentence: The glimmering stars painted a tapestry of light across the midnight sky. Word to encode: glimmering Fractal functions: f_light, f_sparkle, f_beauty Dimensions: 1.6, 2.2, 1.9 Probability weights: 0.4, 0.3, 0.3 Reasoning: Let's think step by step in order to encode the sentence. --- Question: Sentence: The glimmering stars painted a tapestry of light across the midnight sky. Word to encode: glimmering Fractal functions: f_light, f_sparkle, f_beauty Dimensions: 1.6, 2.2, 1.9 Probability weights: 0.4, 0.3, 0.3 Answer: Apply f_light and f_sparkle to powered inputs glimmering^(1/1.6) & glimmering^(1/2.2) & glimmering^(1/1.9). Weight outputs 0.4 and 0.3.\n",
            "\n",
            "---\n",
            "\n",
            "Question: Encapsulate symmetry properties through group actions.\n",
            "Reasoning: Let's think step by step in order to Given the fields `question`, produce the fields `answer`. --- Follow the following format. Question: ${question} Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
            "Answer: ${answer} --- Question: Imagine raising the word to a non-integer power before applying these branches. This allows us to capture nuances, like a gentle breeze for \"delicate\" or a roaring storm for \"powerful.\" Answer: (Model understands the role of dimensions in capturing different levels of meaning.) --- Question: Prove ∃x(¬P(x)) from ∀x(P(x) ⇒ Q(x)) and ∃x(¬Q(x\n",
            "Backward Question: ${question}\n",
            "\n",
            "---\n",
            "\n",
            "Answer: Group Actions: Identify subsets of permissible transformations operating on the collection of symbols, preserving structure and relationships among members. This includes rotational symmetries, reflections, translations, and other automorphisms.\n",
            "Reasoning: Let's think step by step in order to Given the fields `answer`, produce the fields `question`. --- Follow the following format. Answer: ${answer} Reasoning: Let's think step by step in order to ${produce the question}. We ...\n",
            "Question: ${question} --- Answer: (Model understands the role of dimensions in capturing different levels of meaning.) Question: Imagine raising the word to a non-integer power before applying these branches. This allows us to capture nuances, like a gentle breeze for \"delicate\" or a roaring storm for \"powerful.\" --- Answer: (Model constructs a formal proof using inference rules and assumptions.) Question: Prove ∃x(¬P(x)) from ∀x(P(x) ⇒ Q(x)) and ∃x(¬Q(x)) using natural deduction. --- Answer: \"Cold start issues arise in recommendation systems when insufficient user history impedes accurate personalization. Integrating the P-FAF function can alleviate these difficulties by providing a flexible framework for injecting auxiliary information into otherwise sparse interaction matrices. For instance, side information pertaining to item categories, user demographics, or external knowledge graphs can be incorporated via tailored fractal functions. Consequently, the P-FAF function serves two primary purposes in tackling cold start problems: Augmenting scarce historical records with supplementary data sources Encoding heterogeneous information types consistently, regardless of varying availability or density levels By doing so, the P-FAF function contributes to building comprehensive user profiles and item representations, bolstering recommendations even during early stages of engagement.\" Question: What role does the P-FAF function play in addressing cold start issues in recommendation systems? --- Answer: Apply f_regret and f_atonement to powered inputs penitent^(1/1.7) & penitent^(1/1.9). Weight outputs 0.6 and 0.4. Sum to encode. Question: Word: penitent Functions: f_regret, f_atonement Dimensions: 1.7, 1.9 Distribution: 0.6, 0.4 --- Answer: Group Actions: Identify subsets of permissible transformations operating on the collection of symbols, preserving structure and relationships among members. This includes rotational symmetries, reflections, translations, and other automorphisms. Reasoning: Let's think step by step in order to group actions. We can group actions into three categories: - Rotations: Rotating the entire collection of symbols by a fixed angle around a fixed point. - Reflections: Reflecting the entire collection of symbols across a fixed line. - Translations: Moving the entire collection of symbols along a fixed direction. --- Answer: (Model constructs a formal proof using inference rules and assumptions.) Question: Prove ∀x(x ∈ {a, b, c}) ⇒ ∃y(y ∈ {a, b, c}) using natural deduction. --- Answer: \"The P-FAF function is a powerful tool for handling cold start issues in recommendation systems.\n",
            "\n",
            "---\n",
            "\n",
            "Answer: Output: Perform calculations similar to Example 1 but now involving more sub-vectors due to different probability distribution values.\n",
            "Reasoning: Let's think step by step in order to Given the fields `answer`, produce the fields `question`. --- Follow the following format. Answer: ${answer} Reasoning: Let's think step by step in order to ${produce the question}. We ...\n",
            "Question: ${question} --- Answer: (Model understands the role of dimensions in capturing different levels of meaning.) Question: Imagine raising the word to a non-integer power before applying these branches. This allows us to capture nuances, like a gentle breeze for \"delicate\" or a roaring storm for \"powerful.\" --- Answer: (Model constructs a formal proof using inference rules and assumptions.) Question: Prove ∃x(¬P(x)) from ∀x(P(x) ⇒ Q(x)) and ∃x(¬Q(x)) using natural deduction. --- Answer: \"Cold start issues arise in recommendation systems when insufficient user history impedes accurate personalization. Integrating the P-FAF function can alleviate these difficulties by providing a flexible framework for injecting auxiliary information into otherwise sparse interaction matrices. For instance, side information pertaining to item categories, user demographics, or external knowledge graphs can be incorporated via tailored fractal functions. Consequently, the P-FAF function serves two primary purposes in tackling cold start problems: Augmenting scarce historical records with supplementary data sources Encoding heterogeneous information types consistently, regardless of varying availability or density levels By doing so, the P-FAF function contributes to building comprehensive user profiles and item representations, bolstering recommendations even during early stages of engagement.\" Question: What role does the P-FAF function play in addressing cold start issues in recommendation systems? --- Answer: Apply f_regret and f_atonement to powered inputs penitent^(1/1.7) & penitent^(1/1.9). Weight outputs 0.6 and 0.4. Sum to encode. Question: Word: penitent Functions: f_regret, f_atonement Dimensions: 1.7, 1.9 Distribution: 0.6, 0.4 --- Answer: Output: Perform calculations similar to Example 1 but now involving more sub-vectors due to different probability distribution values. Reasoning: Let's think step by step in order to produce the output. --- Answer: (Model constructs a formal proof using inference rules and assumptions.) Question: Prove ∃x(¬P(x)) from ∀x(P(x) ⇒ Q(x)) and ∃x(¬Q(x)) using natural deduction. --- Answer: \"Cold start issues arise in recommendation systems when insufficient user history impedes accurate personalization. Integrating the P-FAF function can alleviate these difficulties by providing a flexible framework for injecting auxiliary information into otherwise sparse interaction matrices. For instance, side information pertaining to item categories, user demographics, or external knowledge graphs can be incorporated via tailored fractal functions. Consequently, the P-FA\n",
            "\n",
            "---\n",
            "\n",
            "Answer: **Learning Path: Website Design** **Step 1: Learn the basics of HTML and CSS.** HTML is the markup language used to create the structure of a website. CSS is the style sheet language used to control the appearance of a website. You can learn the basics of HTML and CSS by taking online courses, reading tutorials, or working through books. **Step 2: Choose a website builder or content management system (CMS).** A website builder or CMS is a software application that makes it easy to create and manage a website without having to know any code. There are many different website builders and CMSs available, so you should choose one that is easy to use and that meets your needs. **Step 3: Design your website.** Once you have chosen a website builder or CMS, you can start designing your website. You can use a pre-made template or create your own custom design. When designing your website, keep in mind the purpose of your website and the target audience you are trying to reach. **Step 4: Add content to your website.** Once you have designed your website, you can start adding content. This can include text, images, videos, and other media. When adding content to your website, make sure it is relevant to your target audience and that it is well-written and error-free. **Step 5: Promote your website.** Once your website is up and running, you need to promote it so that people can find it. You can promote your website by using social media, search engine optimization (SEO), and paid advertising. **Experiment:** Once you have completed your website, you can conduct an experiment to test the effectiveness of your design and content. You can track the number of visitors to your website, the amount of time they spend on your website, and the actions they take on your website.\n",
            "Reasoning: Let's think step by step in order to Given the fields `answer`, produce the fields `question`. --- Follow the following format. Answer: ${answer} Reasoning: Let's think step by step in order to ${produce the question}. We ...\n",
            "Question: ${question} --- Answer: (Model understands the role of dimensions in capturing different levels of meaning.) Question: Imagine raising the word to a non-integer power before applying these branches. This allows us to capture nuances, like a gentle breeze for \"delicate\" or a roaring storm for \"powerful.\" --- Answer: (Model constructs a formal proof using inference rules and assumptions.) Question: Prove ∃x(¬P(x)) from ∀x(P(x) ⇒ Q(x)) and ∃x(¬Q(x)) using natural deduction. --- Answer: \"Cold start issues arise in recommendation systems when insufficient user history impedes accurate personalization. Integrating the P-FAF function can alleviate these difficulties by providing a flexible framework for injecting auxiliary information into otherwise sparse interaction matrices. For instance, side information pertaining to item categories, user demographics, or external knowledge graphs can be incorporated via tailored fractal functions. Consequently, the P-FAF function serves two primary purposes in tackling cold start problems: Augmenting scarce historical records with supplementary data sources Encoding heterogeneous information types consistently, regardless of varying availability or density levels By doing so, the P-FAF function contributes to building comprehensive user profiles and item representations, bolstering recommendations even during early stages of engagement.\" Question: What role does the P-FAF function play in addressing cold start issues in recommendation systems? --- Answer: Apply f_regret and f_atonement to powered inputs penitent^(1/1.7) & penitent^(1/1.9). Weight outputs 0.6 and 0.4. Sum to encode. Question: Word: penitent Functions: f_regret, f_atonement Dimensions: 1.7, 1.9 Distribution: 0.6, 0.4 --- Answer: **Learning Path: Website Design** **Step 1: Learn the basics of HTML and CSS.** HTML is the markup language used to create the structure of a website. CSS is the style sheet language used to control the appearance of a website. You can learn the basics of HTML and CSS by taking online courses, reading tutorials, or working through books. **Step 2: Choose a website builder or content management system (CMS).** A website builder or CMS is a software application that makes it easy to create and manage a website without having to know any code. There are many different website builders and CMSs available, so you should choose one that is easy to use and that meets your needs. **Step 3: Design your website.** Once you have chosen a website builder or CMS, you can start designing your website. You can use a pre-made template or create your own custom design. When designing your website, keep in mind the purpose of your website and the target audience you are trying to reach. **Step 4: Add content to your website.** Once you have designed your website, you can start adding content. This can include text, images, videos, and other media. When adding content to your website, make sure it is relevant to your target audience and that it is well-written and error-free. **Step 5: Promote your website.** Once your website is up and running, you need to promote it so that people can find it. You can promote your website by using social media, search engine optimization (SEO), and paid advertising. **Experiment:** Once you have completed your website, you can conduct an experiment to test the effectiveness of your design and content. You can track the number of visitors to your website, the amount of time they spend on your website, and the actions they take on your website. Reasoning: Let's think step by step in order to ${produce the question}. We can learn the basics of HTML and CSS by taking online courses, reading tutorials, or working through books. We can choose a website builder or CMS. We can design our website. We can add content to our website. We can promote our website. We can experiment. --- Answer: (Model constructs a formal proof using inference rules and assumptions.) Question: Prove ∃x(¬P(x)) from ∀x(P(x) ⇒ Q(x)) and ∃x(¬Q(x)) using natural deduction. --- Answer: \"Cold start issues arise in recommendation systems when insufficient user history impedes accurate personalization.\n",
            "\n",
            "---\n",
            "\n",
            "Answer: Apply f_light(glimmering^(1/1.6)), f_sparkle(glimmering^(1/2.2)), and f_beauty(glimmering^(1/1.9)). Weight outputs 0.4, 0.3, and 0.3. Sum weighted outputs to get P-FAF encoding vector.\n",
            "Reasoning: Let's think step by step in order to Given the fields `answer`, produce the fields `question`. --- Follow the following format. Answer: ${answer} Reasoning: Let's think step by step in order to ${produce the question}. We ...\n",
            "Question: ${question} --- Answer: (Model understands the role of dimensions in capturing different levels of meaning.) Question: Imagine raising the word to a non-integer power before applying these branches. This allows us to capture nuances, like a gentle breeze for \"delicate\" or a roaring storm for \"powerful.\" --- Answer: (Model constructs a formal proof using inference rules and assumptions.) Question: Prove ∃x(¬P(x)) from ∀x(P(x) ⇒ Q(x)) and ∃x(¬Q(x)) using natural deduction. --- Answer: \"Cold start issues arise in recommendation systems when insufficient user history impedes accurate personalization. Integrating the P-FAF function can alleviate these difficulties by providing a flexible framework for injecting auxiliary information into otherwise sparse interaction matrices. For instance, side information pertaining to item categories, user demographics, or external knowledge graphs can be incorporated via tailored fractal functions. Consequently, the P-FAF function serves two primary purposes in tackling cold start problems: Augmenting scarce historical records with supplementary data sources Encoding heterogeneous information types consistently, regardless of varying availability or density levels By doing so, the P-FAF function contributes to building comprehensive user profiles and item representations, bolstering recommendations even during early stages of engagement.\" Question: What role does the P-FAF function play in addressing cold start issues in recommendation systems? --- Answer: Apply f_regret and f_atonement to powered inputs penitent^(1/1.7) & penitent^(1/1.9). Weight outputs 0.6 and 0.4. Sum to encode. Question: Word: penitent Functions: f_regret, f_atonement Dimensions: 1.7, 1.9 Distribution: 0.6, 0.4 --- Answer: Apply f_light(glimmering^(1/1.6)), f_sparkle(glimmering^(1/2.2)), and f_beauty(glimmering^(1/1.9)). Weight outputs 0.4, 0.3, and 0.3. Sum weighted outputs to get P-FAF encoding vector. Reasoning: Let's think step by step in order to produce the P-FAF encoding vector. We can apply the following functions: * f_regret: ${f_regret} * f_light: ${f_light} * f_sparkle: ${f_sparkle} * f_beauty: ${f_beauty} --- Answer: (Model constructs a formal proof using inference rules and assumptions.) Question: Prove ∃x(¬P(x)) from ∀x(P(x) ⇒ Q(x)) and ∃x(¬Q(x)) using natural deduction. --- Answer: \"Cold start issues arise\n",
            "\n",
            "---\n",
            "\n",
            "Answer: Group Actions: Identify subsets of permissible transformations operating on the collection of symbols, preserving structure and relationships among members. This includes rotational symmetries, reflections, translations, and other automorphisms.\n",
            "Reasoning: Let's think step by step in order to Given the fields `answer`, produce the fields `question`. --- Follow the following format. Answer: ${answer} Reasoning: Let's think step by step in order to ${produce the question}. We ...\n",
            "Question: ${question} --- Answer: (Model understands the role of dimensions in capturing different levels of meaning.) Question: Imagine raising the word to a non-integer power before applying these branches. This allows us to capture nuances, like a gentle breeze for \"delicate\" or a roaring storm for \"powerful.\" --- Answer: (Model constructs a formal proof using inference rules and assumptions.) Question: Prove ∃x(¬P(x)) from ∀x(P(x) �\n"
          ]
        }
      ]
    }
  ]
}